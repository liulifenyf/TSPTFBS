The number of train datas: 3330
The number of test datas: 834
epoch	train_loss	train_acc	val_loss	val_acc
0	0.7184086753083421	0.5156156156156156	0.6971928983283557	0.5047961649277227
1	0.7072332155060124	0.506006006006006	0.6899656198865218	0.5335731414868106
2	0.7015512396265436	0.5156156156156156	0.6878663428681646	0.5671462828306844
3	0.6978019758029743	0.5159159159159159	0.685225033931595	0.571942446329039
4	0.6925745359054198	0.527927927927928	0.6844612957476426	0.5599520389410517
5	0.6868635839170164	0.5411411411411411	0.680810278411106	0.5923261369446771
6	0.6894196985004184	0.5399399399399399	0.6785666273652221	0.6151079134546595
7	0.6783701778174163	0.5711711711711712	0.675464763201112	0.6187050339701079
8	0.6821220218598306	0.5633633633633633	0.6731226887348458	0.6306954435021471
9	0.6737872526452348	0.5825825825825826	0.6689905312707384	0.6438848909428365
10	0.6686631261049448	0.5876876876876876	0.6623121427117492	0.6534772172248621
11	0.6645198010825538	0.603003003003003	0.6563756091417454	0.6690647484873125
12	0.6596469849079579	0.6078078078078079	0.6530074773074912	0.6750599524671796
13	0.644579110167048	0.6438438438438439	0.646815936914165	0.6690647460573869
14	0.646575132146612	0.6285285285285285	0.639486225103017	0.6834532368383247
15	0.634994739168757	0.6606606606606606	0.6330979962428983	0.6954436457986168
16	0.6203485502495064	0.6771771771771772	0.6228029739370735	0.6894484413899392
17	0.612326538276386	0.675075075075075	0.6107353833939532	0.7134292550224195
18	0.599804423592828	0.6867867867867868	0.5939357879636384	0.7362110330332383
19	0.5798947152432736	0.7144144144144144	0.5790555106936027	0.7589928040401541
20	0.5609583299797218	0.7285285285285286	0.5632499482134263	0.7625899263423124
21	0.5442858170818639	0.751951951951952	0.5488008356494584	0.7613908854319895
22	0.5303038560234391	0.7510510510510511	0.5267640302221266	0.7805755391395349
23	0.5136341921918027	0.7711711711711712	0.5118586162297274	0.7961630681149965
24	0.4931511896508592	0.7900900900900901	0.48769456541223777	0.8069544373084602
25	0.4693158162606729	0.8048048048048048	0.4733461754785167	0.8225419685709104
26	0.4528546427463268	0.8093093093093093	0.4690900094074597	0.8201438847491496
27	0.4339546001709259	0.8177177177177177	0.43743777875420004	0.8405275789382075
28	0.40933924437285185	0.830930930930931	0.41857331662441044	0.8357314132958007
29	0.4004191533312783	0.8333333333333334	0.40650557735555176	0.859712231216385
30	0.377003961926824	0.8516516516516517	0.40788847225890174	0.8477218212555352
31	0.36282334452992804	0.8594594594594595	0.3900950775443793	0.857314147394624
32	0.3387573250063189	0.8744744744744745	0.41667606204533747	0.8477218248289552
33	0.3507776344099918	0.8681681681681682	0.38401909905086035	0.8633093512315544
34	0.33293400790777294	0.8738738738738738	0.3617218461253946	0.870503595835871
35	0.31418741352341556	0.8864864864864865	0.3695090116499711	0.8657074363397466
36	0.31125249964696866	0.8861861861861862	0.34332261368525113	0.8848920850445041
37	0.30461296885758193	0.8963963963963963	0.3433755313178046	0.883693044277118
38	0.2943979231086937	0.893993993993994	0.3398765343056022	0.883693044277118
39	0.2843133526521402	0.9015015015015015	0.3255477171007106	0.8776978438706706
40	0.28267735991034065	0.9024024024024024	0.32807106744471215	0.8776978438706706
41	0.299522995734	0.8924924924924925	0.32827349353751406	0.8800959242619485
42	0.2771599174396054	0.9063063063063063	0.31907434279112507	0.8800959254054429
43	0.2626213958805746	0.9153153153153153	0.3398424331947482	0.8788968812075736
44	0.2515083059221059	0.912012012012012	0.3299515984184164	0.8824940035097318
45	0.25792621217511436	0.9141141141141141	0.3335695743417854	0.8776978404401875
46	0.257052264897315	0.9069069069069069	0.3167938192328103	0.8872901665792763
47	0.24767290060226624	0.9141141141141141	0.32504345733913587	0.8860911258118902
48	0.24395711393327685	0.918018018018018	0.3146358121284764	0.8860911258118902
49	0.2479416576994432	0.9174174174174174	0.3129955614141995	0.8848920850445041
50	0.2367782106404906	0.9234234234234234	0.31208083155057986	0.8848920850445041
51	0.22749741739696927	0.9237237237237237	0.3084156257357243	0.8860911258118902
52	0.23298142615470802	0.9213213213213213	0.30965901264469686	0.8860911258118902
53	0.22646332863751834	0.9204204204204204	0.30918502200135795	0.8860911258118902
54	0.2258298858776465	0.9261261261261261	0.30708898128651313	0.8860911269553845
55	0.22391811258442051	0.927027027027027	0.3035680821759524	0.8872901677227706
56	0.22043385597857626	0.9267267267267267	0.31810232150040085	0.8848920850445041
57	0.21531038990652596	0.9294294294294294	0.3072206270065811	0.8860911269553845
58	0.21755743057833418	0.9315315315315316	0.3071036726522217	0.8884892084901568
59	0.21231903636956717	0.9309309309309309	0.2980894641839057	0.8884892096336511
60	0.20860495717675837	0.9234234234234234	0.29949437649987587	0.887290168866265
61	0.2112458943604707	0.9288288288288288	0.3068400010716715	0.8884892084901568
62	0.2120289529296192	0.9324324324324325	0.29837175446162695	0.8884892096336511
63	0.21202143456663813	0.9294294294294294	0.29574005634521694	0.8896882504010372
64	0.19715406517568115	0.9348348348348349	0.2990672509018466	0.8896882492575429
65	0.19923894446533363	0.9324324324324325	0.30212213888728645	0.8884892073466624
66	0.19844181451711568	0.9336336336336336	0.29620306590478196	0.8884892096336511
67	0.20279657885476993	0.9318318318318318	0.2897028324129484	0.8896882504010372
68	0.20239170922173394	0.9303303303303303	0.2919016864588507	0.8884892096336511
69	0.18851803689439958	0.9375375375375375	0.29539534149410057	0.8896882504010372
70	0.19320239178366488	0.9351351351351351	0.28961629440172687	0.8896882504010372
71	0.185455092713069	0.9327327327327327	0.2906906058748277	0.8896882504010372
72	0.19033285625525065	0.9387387387387387	0.3011436682525013	0.8872901677227706
73	0.18549646409424217	0.93993993993994	0.29486877126373545	0.8884892096336511
74	0.18300316978592773	0.9408408408408409	0.29482570562145405	0.8884892096336511
75	0.18262347863489264	0.9378378378378378	0.2882019223855268	0.8896882504010372
76	0.17975220934406774	0.9414414414414415	0.3007058223588861	0.8872901677227706
77	0.18243391703556966	0.9405405405405406	0.3447290784735188	0.8848920874744296
78	0.18660514863045724	0.9348348348348349	0.2808548253503063	0.8944844146140759
79	0.18764442082818922	0.9378378378378378	0.28455911430237674	0.8896882504010372
80	0.17378448884900627	0.9396396396396396	0.2902709516546995	0.8896882504010372
81	0.1704275237390766	0.9438438438438439	0.28477328872795016	0.8908872911684232
82	0.17205397533224867	0.9423423423423424	0.29371195106054665	0.8896882504010372
83	0.17167113414397828	0.9423423423423424	0.2774315724055544	0.8920863319358094
84	0.17219405208264027	0.9432432432432433	0.31147477888374875	0.8860911293853101
85	0.18081306575073136	0.9369369369369369	0.27903301863790414	0.8908872911684232
86	0.1663945218151038	0.9426426426426426	0.2842244455854384	0.8908872911684232
87	0.1701747746517261	0.9426426426426426	0.2901941096539692	0.8932853715597011
88	0.15794066677580368	0.9513513513513514	0.27687551702955643	0.8956834542379677
89	0.1590885578825309	0.9456456456456457	0.2805581392644407	0.8932853727031955
90	0.16070961594223618	0.9459459459459459	0.275415792096433	0.8944844146140759
91	0.1761839240476176	0.9411411411411411	0.27690146538279325	0.8932853715597011
92	0.16106043944845685	0.9465465465465466	0.28760560350023584	0.8908872911684232
93	0.1525291417096112	0.9519519519519519	0.2856541316715076	0.8932853727031955
94	0.15420589992949912	0.9474474474474475	0.2806998227211497	0.8932853727031955
95	0.14891070970096387	0.9483483483483484	0.2779131102547657	0.8968824950053538
96	0.14717628842180555	0.9531531531531532	0.276041237213057	0.8968824950053538
97	0.14599604775329253	0.9513513513513514	0.28059770406293066	0.8920863319358094
98	0.14265196337137376	0.9507507507507508	0.2789343365376516	0.8956834542379677
99	0.14252656139097772	0.9522522522522523	0.2901889725411824	0.8944844134705816

The optimal condition:
	epoch: 96
	train_acc: 0.9531531531531532
	val_acc: 0.896882495005
	using time: 237.968163013
