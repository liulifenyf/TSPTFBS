The number of train datas: 3840
The number of test datas: 960
epoch	train_loss	train_acc	val_loss	val_acc
0	0.6973948438962301	0.52265625	0.6903576532999675	0.5479166666666667
1	0.6949721515178681	0.52578125	0.6866408109664917	0.5666666666666667
2	0.6939844926198323	0.52265625	0.6833125114440918	0.5875
3	0.6864985704421998	0.55546875	0.6800000905990601	0.6020833333333333
4	0.6840202112992605	0.5546875	0.6762926737467448	0.621875
5	0.6786589821179708	0.57265625	0.6729511578877767	0.6354166666666666
6	0.6757352471351623	0.5739583333333333	0.6684828042984009	0.6572916666666667
7	0.6727686564127604	0.58125	0.6636256019274394	0.6645833333333333
8	0.6656471431255341	0.6026041666666667	0.6578001737594604	0.6989583333333333
9	0.65797278881073	0.62109375	0.6500916878382365	0.7
10	0.6544769744078318	0.6330729166666667	0.6417447686195373	0.7145833333333333
11	0.646505453189214	0.64296875	0.6325310349464417	0.740625
12	0.63838037053744	0.6473958333333333	0.621930734316508	0.753125
13	0.6253477295239767	0.66875	0.6100768645604452	0.759375
14	0.6190194368362427	0.6755208333333333	0.598297921816508	0.7677083333333333
15	0.6060056885083517	0.68828125	0.5837336341540019	0.7760416666666666
16	0.5886278072992961	0.7109375	0.5657577912012736	0.7989583333333333
17	0.5759095648924509	0.7229166666666667	0.5480478743712107	0.8052083333333333
18	0.5534736335277557	0.7419270833333333	0.5263712088267009	0.8197916666666667
19	0.537327253818512	0.7549479166666667	0.5045527080694835	0.8291666666666667
20	0.5177377631266912	0.7703125	0.4863241930802663	0.8177083333333334
21	0.49955982764561974	0.7872395833333333	0.4567808429400126	0.86875
22	0.4705446978410085	0.7950520833333333	0.4287821074326833	0.8760416666666667
23	0.45688345829645793	0.8091145833333333	0.4016593853632609	0.890625
24	0.4172543595234553	0.8377604166666667	0.37001256545384725	0.8989583333333333
25	0.39639044205347695	0.8471354166666667	0.34157292048136395	0.909375
26	0.36605433026949563	0.8666666666666667	0.3136441965897878	0.9145833333333333
27	0.3450579732656479	0.8760416666666667	0.2823587656021118	0.9291666666666667
28	0.3149145722389221	0.89296875	0.25385164221127826	0.9385416666666667
29	0.2898400311668714	0.903125	0.22867265939712525	0.940625
30	0.2678093507885933	0.91015625	0.2087206502755483	0.95
31	0.24931881924470264	0.9208333333333333	0.1890441874663035	0.9572916666666667
32	0.22824568698803585	0.9333333333333333	0.17220355520645778	0.9625
33	0.21153467347224553	0.9395833333333333	0.15690052608648936	0.9645833333333333
34	0.18815673440694808	0.946875	0.14560682127873104	0.9645833333333333
35	0.18525976041952769	0.9455729166666667	0.14110679179430008	0.9677083333333333
36	0.17059013197819392	0.95078125	0.12686828474203746	0.9666666666666667
37	0.15652286633849144	0.9567708333333333	0.1195832629998525	0.9666666666666667
38	0.15431831032037735	0.9588541666666667	0.11593254307905833	0.9697916666666667
39	0.1454726830124855	0.9609375	0.10853214214245478	0.971875
40	0.13452088783184687	0.9638020833333333	0.10576760917901992	0.9708333333333333
41	0.14081380839149157	0.9640625	0.1015547014772892	0.9729166666666667
42	0.12733154396216076	0.9684895833333333	0.09864480942487716	0.975
43	0.11673744271198909	0.96875	0.09598520249128342	0.9760416666666667
44	0.11749212605257829	0.9684895833333333	0.0918983481824398	0.9739583333333334
45	0.12083859393994013	0.9690104166666667	0.09116767073671023	0.9760416666666667
46	0.10875273272395133	0.975	0.09095798134803772	0.9770833333333333
47	0.1057917947570483	0.975	0.08635400012135505	0.9770833333333333
48	0.10933483131229878	0.97578125	0.08667262817422548	0.9791666666666666
49	0.09893620560566584	0.9776041666666667	0.08386686003456513	0.9791666666666666
50	0.10468081136544545	0.9755208333333333	0.08274650226036707	0.98125
51	0.0955471302072207	0.9778645833333334	0.08138470786313216	0.978125
52	0.10262847865621248	0.9760416666666667	0.0807420739904046	0.9822916666666667
53	0.09486725007494291	0.97734375	0.07974506467580796	0.98125
54	0.09544361506899197	0.9783854166666667	0.07974023558199406	0.9833333333333333
55	0.09079999650518099	0.9752604166666666	0.07856382355093956	0.978125
56	0.09123989641666412	0.97890625	0.07874033190310001	0.9833333333333333
57	0.09002539652089278	0.9791666666666666	0.0772356408337752	0.9802083333333333
58	0.08212614121536414	0.98359375	0.07711798362433911	0.9802083333333333
59	0.08694351762533188	0.9768229166666667	0.0783308543264866	0.9822916666666667
60	0.08249422125518321	0.9841145833333333	0.07804817128926515	0.98125
61	0.08576971298704544	0.9783854166666667	0.07519337212045987	0.9822916666666667
62	0.08329765945672989	0.9815104166666667	0.07536583418647448	0.9822916666666667
63	0.08590274415910244	0.98203125	0.07787985789279143	0.9802083333333333
64	0.07687249133984247	0.9822916666666667	0.07580525539815426	0.9822916666666667
65	0.08378390520811081	0.9825520833333333	0.07771811733643214	0.9802083333333333
66	0.07684087318678696	0.98046875	0.07387134792904058	0.98125
67	0.07768227166185776	0.9825520833333333	0.07865035521487394	0.9802083333333333
68	0.07774695182840029	0.98125	0.07498353409270446	0.98125
69	0.07314949265370767	0.98515625	0.07394838333129883	0.98125
70	0.07729719902078311	0.9817708333333334	0.07362589960296949	0.98125
71	0.07161577418446541	0.98515625	0.07178570569182435	0.9822916666666667
72	0.07079432122409343	0.9848958333333333	0.0735060149182876	0.98125
73	0.06768871831397215	0.9854166666666667	0.07415657403568426	0.98125
74	0.07224715277552604	0.98203125	0.07172062459091345	0.9822916666666667
75	0.06932056099176406	0.984375	0.07365929391235113	0.98125
76	0.06865580926338831	0.9856770833333334	0.07656347323209048	0.9802083333333333
77	0.07120697163045406	0.9825520833333333	0.0713921302308639	0.98125
78	0.06435074899345636	0.9848958333333333	0.07226543575525284	0.9802083333333333
79	0.07276771155496438	0.9841145833333333	0.07411936540156602	0.98125
80	0.06650741702566544	0.98671875	0.0742917833228906	0.98125
81	0.06940162268777689	0.98671875	0.07387437137464682	0.98125
82	0.06385758618513743	0.9854166666666667	0.07075696376462777	0.98125
83	0.06515815431873004	0.9854166666666667	0.07347192112356424	0.98125
84	0.060195248511930304	0.9846354166666667	0.07022366200884184	0.98125
85	0.06604408820470174	0.9848958333333333	0.07787451992432276	0.9791666666666666
86	0.061482591554522514	0.9846354166666667	0.07211032019307216	0.9802083333333333
87	0.0620678856347998	0.984375	0.07049469302097956	0.9802083333333333
88	0.06119947029898564	0.9846354166666667	0.07028621006757022	0.98125
89	0.06102793533354998	0.9846354166666667	0.07378797555963199	0.9802083333333333
90	0.06107735689729452	0.9856770833333334	0.07107898813361922	0.9802083333333333
91	0.05803775408615668	0.98515625	0.0699430332519114	0.9802083333333333
92	0.058686898462474345	0.9872395833333333	0.0758528663466374	0.9791666666666666
93	0.05793331793198983	0.9872395833333333	0.07144987769424915	0.9802083333333333
94	0.05884924164662759	0.9877604166666667	0.07107468144968151	0.9802083333333333
95	0.05801069295654694	0.9861979166666667	0.07043242094417414	0.9802083333333333
96	0.05797735291222731	0.9859375	0.07318133270988862	0.9791666666666666
97	0.05306672193109989	0.98515625	0.07211299066742262	0.9802083333333333
98	0.057617988189061484	0.98671875	0.07147364417711893	0.9802083333333333
99	0.05460316610212127	0.9869791666666666	0.06933492546280225	0.98125

The optimal condition:
	epoch: 56
	train_acc: 0.97890625
	val_acc: 0.983333333333
	using time: 267.930588007
