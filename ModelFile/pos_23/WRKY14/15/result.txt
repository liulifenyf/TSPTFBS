The number of train datas: 3840
The number of test datas: 960
epoch	train_loss	train_acc	val_loss	val_acc
0	0.7005673428376515	0.5005208333333333	0.6932390650113424	0.48854166666666665
1	0.6968304773171743	0.5203125	0.6903389732042948	0.5322916666666667
2	0.694404137134552	0.50703125	0.6873589674631755	0.5416666666666666
3	0.691192889213562	0.5231770833333333	0.6853056788444519	0.5770833333333333
4	0.6878150562445323	0.55	0.6830291668574016	0.590625
5	0.6842494150002797	0.5528645833333333	0.6811458190282186	0.603125
6	0.6839836418628693	0.5596354166666667	0.678262488047282	0.6135416666666667
7	0.6815333704153697	0.5578125	0.6753538886706034	0.6364583333333333
8	0.6758475065231323	0.5802083333333333	0.6717179854710896	0.6510416666666666
9	0.6717885295550029	0.59375	0.6673045476277669	0.6479166666666667
10	0.6688569068908692	0.5966145833333333	0.6619139512379965	0.6802083333333333
11	0.6631793816884358	0.61953125	0.6559451182683309	0.6927083333333334
12	0.6591718037923177	0.6158854166666666	0.6490060210227966	0.7052083333333333
13	0.6526817182699839	0.6239583333333333	0.6415741761525472	0.7125
14	0.6456723570823669	0.6408854166666667	0.6335030198097229	0.7270833333333333
15	0.6382379492123922	0.6489583333333333	0.6232515335083008	0.7354166666666667
16	0.6262637794017791	0.6630208333333333	0.6103771368662516	0.7572916666666667
17	0.6182120303312938	0.6776041666666667	0.5964905103047689	0.771875
18	0.6002718190352122	0.7015625	0.5806753834088644	0.7927083333333333
19	0.5923955698808034	0.6958333333333333	0.5644067565600077	0.7895833333333333
20	0.5755800028642019	0.728125	0.5515189051628113	0.76875
21	0.5661123792330424	0.7325520833333333	0.5320739686489105	0.7989583333333333
22	0.5397483587265015	0.753125	0.510557496547699	0.8104166666666667
23	0.529302355647087	0.7520833333333333	0.49006818532943724	0.8270833333333333
24	0.5036485572655995	0.7809895833333333	0.4691716134548187	0.83125
25	0.4887508640686671	0.79140625	0.44836833675702414	0.8458333333333333
26	0.46771315634250643	0.7989583333333333	0.42615242799123126	0.8572916666666667
27	0.4472841084003448	0.8088541666666667	0.40057422320048014	0.8729166666666667
28	0.4264449288447698	0.8231770833333333	0.3754483699798584	0.8854166666666666
29	0.39920443296432495	0.8440104166666667	0.3476020932197571	0.8947916666666667
30	0.3735542366902033	0.8559895833333333	0.32246832152207694	0.90625
31	0.3563025385141373	0.8630208333333333	0.2979907602071762	0.9177083333333333
32	0.3320822606484095	0.8747395833333333	0.27445484002431236	0.9270833333333334
33	0.3144140362739563	0.8895833333333333	0.25294169584910076	0.9375
34	0.27843092431624733	0.9083333333333333	0.23241791327794392	0.9427083333333334
35	0.2633150815963745	0.9135416666666667	0.2164371967315674	0.9416666666666667
36	0.2504062846302986	0.9138020833333333	0.19830856422583262	0.9552083333333333
37	0.2222573310136795	0.93125	0.18228284617265064	0.95625
38	0.2149533232053121	0.9299479166666667	0.17072163720925648	0.9583333333333334
39	0.20688621600468954	0.9359375	0.16097872654596965	0.959375
40	0.18818464130163193	0.9471354166666667	0.14987666308879852	0.9614583333333333
41	0.18366817981004716	0.9458333333333333	0.1485804150501887	0.9635416666666666
42	0.16749214058121045	0.9552083333333333	0.135096933444341	0.9666666666666667
43	0.16044097542762756	0.9541666666666667	0.12992294828097026	0.9666666666666667
44	0.15862363850076994	0.9528645833333333	0.12537599702676136	0.9677083333333333
45	0.15331843992074332	0.9580729166666667	0.12217052678267161	0.9697916666666667
46	0.14287038172284763	0.9598958333333333	0.11755750228961309	0.9697916666666667
47	0.13706689948836961	0.96328125	0.1128252478937308	0.9708333333333333
48	0.13542726760109267	0.9609375	0.10987514754136403	0.9708333333333333
49	0.12638442342480025	0.9658854166666667	0.10717998892068863	0.971875
50	0.12741366848349572	0.9653645833333333	0.10398404647906621	0.9708333333333333
51	0.1200850108017524	0.9669270833333333	0.10396839193999767	0.971875
52	0.12016365689535936	0.96875	0.10051821867624919	0.9739583333333334
53	0.11537789218127728	0.9705729166666667	0.09883078162868818	0.9729166666666667
54	0.11388479049007098	0.971875	0.09824605410297711	0.9739583333333334
55	0.10828432763616244	0.9729166666666667	0.09662838305036227	0.9739583333333334
56	0.10780567924181621	0.9723958333333333	0.09504079942901929	0.971875
57	0.1060134656727314	0.9744791666666667	0.09384027409056822	0.9739583333333334
58	0.09708663833638033	0.9739583333333334	0.09218031093478203	0.975
59	0.09734851034979025	0.9744791666666667	0.0919516413162152	0.9729166666666667
60	0.09437647325297197	0.978125	0.09071464116374651	0.975
61	0.09699756242334842	0.9736979166666667	0.0907481387257576	0.975
62	0.09798022707303365	0.9765625	0.08968410069743792	0.975
63	0.09721159612139066	0.9768229166666667	0.08942986192802588	0.975
64	0.08777511765559515	0.9796875	0.08881874196231365	0.975
65	0.09190976272026698	0.9778645833333334	0.08890370180209478	0.9729166666666667
66	0.08738108637432257	0.9794270833333333	0.08717506577571234	0.975
67	0.08766071423888207	0.97890625	0.09030452258884906	0.971875
68	0.08548775638143222	0.97734375	0.08719450533390045	0.975
69	0.07836934520552556	0.98203125	0.08607584461569787	0.9760416666666667
70	0.08749080225825309	0.9776041666666667	0.08786358113090197	0.9739583333333334
71	0.07641157681743303	0.98046875	0.08534975939740737	0.9770833333333333
72	0.07788289847473304	0.9815104166666667	0.08567260305086771	0.975
73	0.08063819278031588	0.9817708333333334	0.08517956708868345	0.9770833333333333
74	0.07867279599110286	0.9802083333333333	0.08537757645050685	0.975
75	0.07415737019230922	0.98203125	0.0873543992638588	0.975
76	0.0786211568241318	0.9830729166666666	0.0866832461208105	0.9729166666666667
77	0.07433787335952123	0.9815104166666667	0.08442138222356638	0.9770833333333333
78	0.06800277717411518	0.98359375	0.08534482220808665	0.9760416666666667
79	0.07496131366739671	0.984375	0.08432276360690594	0.9770833333333333
80	0.07197826796521743	0.9841145833333333	0.08462724847098192	0.9770833333333333
81	0.07321236518522103	0.984375	0.08486436208089193	0.9760416666666667
82	0.07041231275846561	0.9833333333333333	0.08392509718736013	0.9770833333333333
83	0.07268561944365501	0.9846354166666667	0.08390439233432213	0.9770833333333333
84	0.06161410454660654	0.9846354166666667	0.0839202189197143	0.9770833333333333
85	0.07088486204544703	0.9809895833333333	0.08518887497484684	0.9739583333333334
86	0.0659032936518391	0.9854166666666667	0.08361092153936625	0.9770833333333333
87	0.06646706908941269	0.9841145833333333	0.08343394473195076	0.978125
88	0.06332527933021387	0.984375	0.08406236345569293	0.9770833333333333
89	0.0646200534577171	0.98515625	0.08561177477240563	0.975
90	0.062440386166175206	0.9854166666666667	0.0843305287261804	0.975
91	0.061714413948357104	0.9833333333333333	0.08398274512340625	0.9770833333333333
92	0.06400512500355642	0.9838541666666667	0.08418219865610202	0.9760416666666667
93	0.060016363052030404	0.9864583333333333	0.08300217458357413	0.978125
94	0.06286595445126295	0.9854166666666667	0.08340321443974971	0.9770833333333333
95	0.05825838266561429	0.9854166666666667	0.08464972116053104	0.9760416666666667
96	0.05947043454895417	0.9869791666666666	0.08437613186736902	0.9770833333333333
97	0.055124671384692195	0.98671875	0.08444985809425513	0.9770833333333333
98	0.0581353724313279	0.9856770833333334	0.08382145781069994	0.9760416666666667
99	0.05756557211279869	0.9872395833333333	0.08395329105357328	0.9760416666666667

The optimal condition:
	epoch: 93
	train_acc: 0.9864583333333333
	val_acc: 0.978125
	using time: 298.201891899
