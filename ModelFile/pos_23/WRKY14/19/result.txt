The number of train datas: 3840
The number of test datas: 960
epoch	train_loss	train_acc	val_loss	val_acc
0	0.7046498437722524	0.5083333333333333	0.6940161546071371	0.4979166666666667
1	0.6985815207163493	0.5143229166666666	0.6906043450037639	0.5260416666666666
2	0.6960532387097677	0.5013020833333334	0.6883559505144755	0.553125
3	0.6910661657651266	0.5276041666666667	0.6864078601201375	0.55
4	0.6873925189177196	0.5302083333333333	0.6836107651392619	0.571875
5	0.6846418420473734	0.5505208333333333	0.6816961884498596	0.58125
6	0.6833206395308177	0.5588541666666667	0.6785898923873901	0.6020833333333333
7	0.6801037073135376	0.5567708333333333	0.6756044387817383	0.61875
8	0.6745243867238363	0.584375	0.6718863646189371	0.6354166666666666
9	0.6700439592202504	0.5986979166666667	0.6667266368865967	0.6385416666666667
10	0.6680990378061931	0.5934895833333333	0.6615642706553141	0.6604166666666667
11	0.6617643078168233	0.6130208333333333	0.6562716841697693	0.6708333333333333
12	0.6572333017985026	0.6184895833333334	0.6497408946355184	0.6895833333333333
13	0.6511051754156748	0.6299479166666667	0.6428203860918681	0.690625
14	0.6456627726554871	0.6416666666666667	0.6348367253939311	0.7072916666666667
15	0.6375410536924998	0.6546875	0.6259313583374023	0.7135416666666666
16	0.6277886430422465	0.6700520833333333	0.6139968395233154	0.74375
17	0.6241415441036224	0.6760416666666667	0.6034168759981792	0.74375
18	0.6048486451307933	0.6833333333333333	0.5904367446899415	0.7447916666666666
19	0.5997487723827362	0.6971354166666667	0.5790188670158386	0.753125
20	0.5917173802852631	0.6958333333333333	0.5723068376382192	0.7229166666666667
21	0.5813610792160034	0.7135416666666666	0.5573419133822123	0.75625
22	0.5653546273708343	0.7265625	0.5429075976212819	0.7697916666666667
23	0.5544095655282338	0.7328125	0.5282889286677043	0.7770833333333333
24	0.5389707247416179	0.7354166666666667	0.5157396733760834	0.775
25	0.5335758874813715	0.7453125	0.5016291379928589	0.79375
26	0.5186134527126948	0.7528645833333333	0.48828233281771344	0.7947916666666667
27	0.5001213798920313	0.76640625	0.47172428568204244	0.8
28	0.48621693054835	0.7848958333333333	0.4538157860438029	0.8260416666666667
29	0.46475281020005543	0.7911458333333333	0.4314663092295329	0.8354166666666667
30	0.4469958901405334	0.8028645833333333	0.4126350184281667	0.840625
31	0.4286736100912094	0.8252604166666667	0.38718026677767436	0.8625
32	0.40592319071292876	0.8328125	0.3646364708741506	0.875
33	0.39192441403865813	0.8416666666666667	0.343134464820226	0.8739583333333333
34	0.3543203383684158	0.8638020833333333	0.3163583715756734	0.8979166666666667
35	0.3340115676323573	0.87890625	0.2938444753487905	0.9052083333333333
36	0.31483516295750935	0.884375	0.27160421510537464	0.9291666666666667
37	0.2844709545373917	0.903125	0.24763538042704264	0.9416666666666667
38	0.27035433053970337	0.9088541666666666	0.22766838868459066	0.9489583333333333
39	0.25269339929024376	0.91640625	0.21128043830394744	0.9520833333333333
40	0.2333803157011668	0.9236979166666667	0.19516795476277668	0.9572916666666667
41	0.21940616170565289	0.9369791666666667	0.1856846034526825	0.9552083333333333
42	0.20176187952359517	0.9377604166666667	0.169532976547877	0.959375
43	0.190276766816775	0.9424479166666667	0.15752083311478296	0.9645833333333333
44	0.1820117488503456	0.9450520833333333	0.1484641934434573	0.965625
45	0.17418600966533024	0.9510416666666667	0.14006851216157276	0.9697916666666667
46	0.15467095002532005	0.9583333333333334	0.13205069502194722	0.9697916666666667
47	0.151147856314977	0.95625	0.12651609554886817	0.9677083333333333
48	0.14834141706426937	0.95703125	0.1207649124165376	0.9677083333333333
49	0.1391256754597028	0.9609375	0.11573115140199661	0.971875
50	0.1376654143134753	0.9666666666666667	0.11244325240453085	0.9708333333333333
51	0.12669159000118574	0.9627604166666667	0.10948507015903791	0.9729166666666667
52	0.12900225594639778	0.96484375	0.10544596066077551	0.9729166666666667
53	0.11419660076498986	0.9734375	0.10141340643167496	0.971875
54	0.11741360997160276	0.9690104166666667	0.09945726034541925	0.9729166666666667
55	0.11168090254068375	0.9723958333333333	0.0965864684432745	0.971875
56	0.10561983759204546	0.9731770833333333	0.095585352430741	0.975
57	0.10521390636761983	0.9723958333333333	0.09338875375688076	0.975
58	0.10100418043633302	0.9731770833333333	0.09026764445006848	0.975
59	0.09588424389561018	0.9736979166666667	0.08983955557147662	0.975
60	0.09327379440267881	0.978125	0.08812133508423964	0.9760416666666667
61	0.09877562001347542	0.9747395833333333	0.086705523605148	0.9739583333333334
62	0.09843403734266758	0.97578125	0.08578932502617438	0.9739583333333334
63	0.09460205944875875	0.97734375	0.08561754698554674	0.9770833333333333
64	0.08543074714640776	0.978125	0.08479168464740118	0.9770833333333333
65	0.09273890381058057	0.9770833333333333	0.08484147091706594	0.978125
66	0.08607532369593779	0.97890625	0.08222723013410965	0.9770833333333333
67	0.08441847388943037	0.9791666666666666	0.08836496782799562	0.9729166666666667
68	0.07861845841010412	0.9799479166666667	0.0805900822704037	0.978125
69	0.08114402530093988	0.9794270833333333	0.08013993489245573	0.978125
70	0.08157560937106609	0.98046875	0.07986868284642697	0.978125
71	0.0721606166412433	0.9846354166666667	0.07805011949191491	0.9791666666666666
72	0.0709648985415697	0.9833333333333333	0.08148047973712286	0.9770833333333333
73	0.07276523609956105	0.98359375	0.07959521835048994	0.9791666666666666
74	0.0731987870608767	0.9833333333333333	0.07604921317348877	0.9791666666666666
75	0.07209490338961283	0.98125	0.077496970196565	0.978125
76	0.07421955081323782	0.9830729166666666	0.07811362482607365	0.978125
77	0.06801060400903225	0.9822916666666667	0.07476523419221243	0.9791666666666666
78	0.062322352826595304	0.98671875	0.07557146971424421	0.978125
79	0.07185694432506959	0.9794270833333333	0.07432866071661313	0.9802083333333333
80	0.0667474264279008	0.9846354166666667	0.0743339927867055	0.9802083333333333
81	0.06784956970562538	0.9830729166666666	0.07625238957504431	0.9770833333333333
82	0.06611607198913892	0.9833333333333333	0.07301688849305113	0.9802083333333333
83	0.06604643228153388	0.9828125	0.07265529471139114	0.978125
84	0.055953475770850976	0.9861979166666667	0.07379349917173386	0.9791666666666666
85	0.06363901458680629	0.984375	0.07646191095312437	0.978125
86	0.05617523193359375	0.9872395833333333	0.07312365947291255	0.9791666666666666
87	0.05996914363155762	0.9838541666666667	0.07411178636054198	0.978125
88	0.05836911195268234	0.9856770833333334	0.07340297487874826	0.978125
89	0.0585780988757809	0.984375	0.0770036385084192	0.9770833333333333
90	0.05791526058067878	0.9854166666666667	0.07260807969917854	0.978125
91	0.05437228952844938	0.9869791666666666	0.07052893449241916	0.9802083333333333
92	0.05370483982066313	0.98671875	0.07027062963073452	0.9802083333333333
93	0.05345820809404055	0.9885416666666667	0.0701623234897852	0.9802083333333333
94	0.0536990928153197	0.9875	0.07242206571002802	0.978125
95	0.05191231506566207	0.9861979166666667	0.06965253402789434	0.9791666666666666
96	0.054119686223566534	0.98515625	0.07241651136428118	0.978125
97	0.04817539509385824	0.98671875	0.06926875269661346	0.9791666666666666
98	0.0509123116110762	0.9880208333333333	0.07263851147145033	0.978125
99	0.04967214086403449	0.9869791666666666	0.0692293304639558	0.9791666666666666

The optimal condition:
	epoch: 93
	train_acc: 0.9885416666666667
	val_acc: 0.980208333333
	using time: 332.123736858
