The number of train datas: 3840
The number of test datas: 960
epoch	train_loss	train_acc	val_loss	val_acc
0	0.7006897846857707	0.5088541666666667	0.6913243055343627	0.5208333333333334
1	0.6941338380177816	0.5153645833333333	0.6884072303771973	0.540625
2	0.6927947441736857	0.5234375	0.6854621569315592	0.5791666666666667
3	0.687814462184906	0.5463541666666667	0.6830947796503702	0.5864583333333333
4	0.6850540220737458	0.5502604166666667	0.6804999589920044	0.5989583333333334
5	0.6812207559744518	0.5723958333333333	0.6780255158742269	0.6020833333333333
6	0.6809196452299754	0.5611979166666666	0.6745548645655314	0.6322916666666667
7	0.6774887184302012	0.571875	0.6710446993509929	0.6354166666666666
8	0.6725752453009287	0.59296875	0.6671616276105244	0.659375
9	0.6664908627669016	0.6109375	0.6619590123494467	0.6697916666666667
10	0.6629127025604248	0.6127604166666667	0.6555123647054036	0.6875
11	0.6569392224152882	0.6208333333333333	0.6484940489133199	0.7
12	0.6498665849367777	0.6364583333333333	0.6397362232208252	0.7197916666666667
13	0.6426563759644827	0.6533854166666667	0.6307232499122619	0.7364583333333333
14	0.6333000481128692	0.6611979166666667	0.6205487569173177	0.7520833333333333
15	0.6253337403138478	0.6744791666666666	0.6079429467519124	0.7645833333333333
16	0.6092878997325897	0.6911458333333333	0.5925043980280559	0.7802083333333333
17	0.6005090713500977	0.7057291666666666	0.5762530922889709	0.8
18	0.5790011286735535	0.7299479166666667	0.5571300983428955	0.815625
19	0.5669674654801686	0.7296875	0.5363911350568136	0.8229166666666666
20	0.546788756052653	0.7559895833333333	0.5181480844815572	0.81875
21	0.5307293901840846	0.7625	0.4907556732495626	0.8489583333333334
22	0.4984043190876643	0.7885416666666667	0.46281571785608927	0.8614583333333333
23	0.4846525808175405	0.7963541666666667	0.4346121768156687	0.8802083333333334
24	0.44975221157073975	0.821875	0.40398114919662476	0.8895833333333333
25	0.42740572889645895	0.8291666666666667	0.37371192971865336	0.903125
26	0.3999587665001551	0.8434895833333333	0.3453479051589966	0.9114583333333334
27	0.37314627766609193	0.8619791666666666	0.31131667296091714	0.9333333333333333
28	0.34691231449445087	0.8713541666666667	0.28013635873794557	0.940625
29	0.3154471039772034	0.89140625	0.24989054302374522	0.9489583333333333
30	0.29205602208773296	0.90390625	0.22760113875071208	0.9520833333333333
31	0.2711737518509229	0.9106770833333333	0.2035590539375941	0.95625
32	0.2510996545354525	0.9229166666666667	0.1815528949101766	0.965625
33	0.2320028970638911	0.92890625	0.16365094135204952	0.96875
34	0.2048571974039078	0.9385416666666667	0.14862356086572012	0.96875
35	0.19686386485894522	0.9411458333333333	0.1439548427859942	0.9708333333333333
36	0.1856986626982689	0.9458333333333333	0.1261248394846916	0.9760416666666667
37	0.16407046914100648	0.9546875	0.1164376179377238	0.975
38	0.16274128556251527	0.9533854166666667	0.11054087628920874	0.9760416666666667
39	0.15601998046040536	0.9552083333333333	0.10321612060070037	0.9770833333333333
40	0.1393746313949426	0.96484375	0.09792394538720449	0.9760416666666667
41	0.14034558460116386	0.9625	0.09321944937109947	0.9802083333333333
42	0.12907964413364728	0.9682291666666667	0.08943768565853437	0.9770833333333333
43	0.12101133118073146	0.9651041666666667	0.08461749802033107	0.978125
44	0.1194505475461483	0.9674479166666666	0.08221910993258158	0.98125
45	0.12261239538590113	0.96640625	0.08167196537057558	0.9822916666666667
46	0.11306633154551188	0.9703125	0.07749876454472542	0.9791666666666666
47	0.10767114857832591	0.9734375	0.0752158502737681	0.98125
48	0.10743018339077631	0.9731770833333333	0.0748340534667174	0.978125
49	0.10081786500910918	0.9776041666666667	0.07197159926096598	0.98125
50	0.10212381308277448	0.9729166666666667	0.07016672492027283	0.98125
51	0.09505439661443234	0.97578125	0.06925936763485273	0.9822916666666667
52	0.0980193638553222	0.97578125	0.06804206843177478	0.98125
53	0.09366025179624557	0.9783854166666667	0.06688621267676353	0.98125
54	0.09158008235196273	0.9778645833333334	0.0661315786341826	0.9833333333333333
55	0.09027643948793411	0.9768229166666667	0.0655900239944458	0.984375
56	0.08782786702116331	0.9796875	0.06485197668274244	0.98125
57	0.08462449846168359	0.9791666666666666	0.06397171976665655	0.98125
58	0.07997582120199999	0.9815104166666667	0.0634125375499328	0.98125
59	0.08099347874522209	0.9817708333333334	0.06234280206263065	0.9833333333333333
60	0.08131374269723893	0.98125	0.06249265583852927	0.9822916666666667
61	0.0796901331593593	0.9776041666666667	0.06142110514144103	0.9833333333333333
62	0.08109075166285037	0.9817708333333334	0.06126280364890893	0.9833333333333333
63	0.08372501159707706	0.9825520833333333	0.06130447934071223	0.9833333333333333
64	0.07202270664274693	0.9828125	0.06141140510638555	0.9822916666666667
65	0.07914178104450305	0.9815104166666667	0.06295910390714804	0.9822916666666667
66	0.07370734314123789	0.9817708333333334	0.06005441086987654	0.9833333333333333
67	0.07709810137748718	0.9796875	0.06357334591448308	0.98125
68	0.07551924008876085	0.98125	0.059303739791115126	0.9833333333333333
69	0.07163927610963583	0.9841145833333333	0.059088125452399255	0.9833333333333333
70	0.07524459200600783	0.9815104166666667	0.05966430430610974	0.984375
71	0.06645222802956899	0.9859375	0.058253955592711765	0.984375
72	0.06657018649081389	0.9841145833333333	0.05869155911107858	0.984375
73	0.06775558640559515	0.9817708333333334	0.060388756170868876	0.9833333333333333
74	0.06914620082825422	0.9828125	0.05723195740332206	0.9854166666666667
75	0.06723839131494364	0.9848958333333333	0.0601297814399004	0.9833333333333333
76	0.06450496160735687	0.98359375	0.06040467830995719	0.9822916666666667
77	0.06629802081733942	0.9838541666666667	0.0571660079061985	0.9854166666666667
78	0.06001490118602912	0.9854166666666667	0.057203499724467595	0.9854166666666667
79	0.06873610032101472	0.98359375	0.057866283754507704	0.9854166666666667
80	0.06443042146662871	0.98515625	0.05902603281040986	0.9833333333333333
81	0.0632071187098821	0.98515625	0.058760743712385495	0.9833333333333333
82	0.061421849640707175	0.9846354166666667	0.05598782418916623	0.9854166666666667
83	0.061965084013839565	0.9859375	0.05836833504339059	0.9854166666666667
84	0.05342583190649748	0.9890625	0.05744540865222613	0.9854166666666667
85	0.06218494325876236	0.9846354166666667	0.0602733638137579	0.9833333333333333
86	0.05818986749897401	0.9861979166666667	0.05660810619592667	0.9854166666666667
87	0.05919163798292478	0.9830729166666666	0.0557142736390233	0.9864583333333333
88	0.05871889979268114	0.9854166666666667	0.05541492955138286	0.9864583333333333
89	0.055641000904142854	0.98515625	0.05776720022161801	0.984375
90	0.054740717510382335	0.9875	0.056845745195945104	0.9854166666666667
91	0.052202256644765535	0.9872395833333333	0.05493126679211855	0.9864583333333333
92	0.05440611789623896	0.9859375	0.056355294833580656	0.9854166666666667
93	0.05064755951364835	0.9877604166666667	0.055847705776492756	0.9854166666666667
94	0.054579271593441565	0.98828125	0.05584131802121798	0.9854166666666667
95	0.05519541297107935	0.9854166666666667	0.05668314136564732	0.984375
96	0.051675328115622206	0.9875	0.05798543294270833	0.9833333333333333
97	0.05002555754035711	0.9864583333333333	0.057520461330811186	0.984375
98	0.05364126982167363	0.98828125	0.05670361667871475	0.984375
99	0.052042188961058854	0.98671875	0.05471858040740093	0.9864583333333333

The optimal condition:
	epoch: 99
	train_acc: 0.98671875
	val_acc: 0.986458333333
	using time: 282.664998055
