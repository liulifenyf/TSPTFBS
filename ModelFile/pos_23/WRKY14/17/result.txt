The number of train datas: 3840
The number of test datas: 960
epoch	train_loss	train_acc	val_loss	val_acc
0	0.7026473820209503	0.5046875	0.6939227104187011	0.5072916666666667
1	0.6966761271158854	0.515625	0.6917897303899129	0.5270833333333333
2	0.6949652592341106	0.52109375	0.6893928289413452	0.5489583333333333
3	0.692422620455424	0.5239583333333333	0.6874495228131612	0.5645833333333333
4	0.6889060914516449	0.5359375	0.6854424754778544	0.5697916666666667
5	0.6854854245980581	0.540625	0.6832837025324504	0.5791666666666667
6	0.685043187936147	0.55390625	0.6811635255813598	0.60625
7	0.681959350903829	0.55625	0.6788291613260905	0.6072916666666667
8	0.6774470090866089	0.5833333333333334	0.6758289019266764	0.6229166666666667
9	0.6751169323921203	0.58046875	0.6722859581311543	0.6291666666666667
10	0.6727676510810852	0.5955729166666667	0.6682918310165405	0.6458333333333334
11	0.6681342780590057	0.6067708333333334	0.6634760697682699	0.6614583333333334
12	0.6640615046024323	0.6140625	0.6576684157053629	0.675
13	0.6599759221076965	0.6125	0.6506535291671753	0.6895833333333333
14	0.6507794698079427	0.6375	0.6430949370066324	0.7020833333333333
15	0.6455563763777415	0.6411458333333333	0.6350260853767395	0.70625
16	0.6364357233047485	0.6580729166666667	0.6245587428410848	0.7333333333333333
17	0.6296745796998342	0.6677083333333333	0.6130185683568319	0.7458333333333333
18	0.6120989282925924	0.6869791666666667	0.6001127243041993	0.7520833333333333
19	0.6059450705846151	0.6934895833333333	0.5865198850631714	0.759375
20	0.591034346818924	0.7075520833333333	0.5755252738793691	0.740625
21	0.583186827103297	0.7098958333333333	0.5581370731194814	0.7666666666666667
22	0.5606335242589314	0.7354166666666667	0.5396355231602986	0.7760416666666666
23	0.5494321584701538	0.7447916666666666	0.5216847658157349	0.7927083333333333
24	0.5286009947458903	0.7557291666666667	0.5029449979464213	0.7989583333333333
25	0.512117271622022	0.7671875	0.48220028082529703	0.8177083333333334
26	0.49268471797307334	0.78515625	0.46078959306081135	0.8229166666666666
27	0.46505324443181356	0.8020833333333334	0.43682040373484293	0.840625
28	0.4483454257249832	0.8130208333333333	0.4150147577126821	0.859375
29	0.42231005330880483	0.8276041666666667	0.3840261995792389	0.86875
30	0.3993203471104304	0.8401041666666667	0.3587231437365214	0.8822916666666667
31	0.3750639230012894	0.8583333333333333	0.3297687848409017	0.9052083333333333
32	0.34978294869263965	0.8669270833333333	0.3032600224018097	0.9125
33	0.329661151766777	0.8854166666666666	0.2780405382315318	0.9291666666666667
34	0.29337062140305836	0.9026041666666667	0.25125421583652496	0.9322916666666666
35	0.27358903686205543	0.9075520833333334	0.23115732272466025	0.9375
36	0.2547704403599103	0.9174479166666667	0.21009347240130108	0.9416666666666667
37	0.22722377628087997	0.92734375	0.19070822844902674	0.9479166666666666
38	0.21525342464447023	0.92890625	0.17620750814676284	0.953125
39	0.20370192776123683	0.9401041666666666	0.16724102099736532	0.9541666666666667
40	0.1856346363822619	0.94296875	0.15472428599993387	0.9572916666666667
41	0.17802206327517828	0.9497395833333333	0.1478916771709919	0.9614583333333333
42	0.1609665813545386	0.9541666666666667	0.13514756460984548	0.9635416666666666
43	0.1521799107392629	0.9557291666666666	0.1275263545413812	0.9645833333333333
44	0.15479354163010914	0.953125	0.12437775308887164	0.9635416666666666
45	0.14376349970698357	0.9591145833333333	0.12018379246195157	0.9666666666666667
46	0.13597742939988772	0.9661458333333334	0.11416808068752289	0.9645833333333333
47	0.12580040271083515	0.9653645833333333	0.10976347227891287	0.9666666666666667
48	0.1294458995262782	0.9640625	0.10789477477471034	0.9666666666666667
49	0.11691238656640053	0.9708333333333333	0.10400356128811836	0.9697916666666667
50	0.11414039731025696	0.9752604166666666	0.1008963331580162	0.96875
51	0.10991059715549151	0.96796875	0.09934065590302149	0.9708333333333333
52	0.10926187510291735	0.9703125	0.0981651060283184	0.9697916666666667
53	0.10147673574586709	0.9747395833333333	0.09788972934087117	0.9697916666666667
54	0.10524495877325535	0.9705729166666667	0.09459068377812703	0.9697916666666667
55	0.09733533536394437	0.9747395833333333	0.0929883303741614	0.9697916666666667
56	0.09713744334876537	0.97109375	0.09154383987188339	0.975
57	0.09736673049628734	0.9736979166666667	0.0919059120118618	0.9729166666666667
58	0.09096662700176239	0.9763020833333333	0.09236842294534048	0.9708333333333333
59	0.08706802849968275	0.9768229166666667	0.09124219280978044	0.9708333333333333
60	0.0805166494101286	0.9783854166666667	0.0873752584680915	0.9760416666666667
61	0.08820434808731079	0.9765625	0.08667606990784407	0.9739583333333334
62	0.09005225809911886	0.9778645833333334	0.08679488375782966	0.9760416666666667
63	0.08763913239041964	0.9765625	0.08702780517439047	0.975
64	0.07939578518271447	0.9809895833333333	0.08442267545809348	0.9770833333333333
65	0.08365008756518363	0.9786458333333333	0.08373003986974557	0.978125
66	0.07759696555634339	0.9791666666666666	0.08272965711851915	0.978125
67	0.07655746936798095	0.98125	0.08790599877635638	0.9739583333333334
68	0.07439446120212476	0.98125	0.08404653544227282	0.9770833333333333
69	0.07619032201667626	0.9807291666666667	0.0826499947036306	0.978125
70	0.07504701366027196	0.9799479166666667	0.08294395084182421	0.9770833333333333
71	0.06779914665967227	0.9817708333333334	0.07972296165923277	0.9760416666666667
72	0.0699600292990605	0.9833333333333333	0.08007645619412264	0.978125
73	0.06613631310562293	0.9838541666666667	0.08175972315172354	0.9760416666666667
74	0.06903058638175329	0.9815104166666667	0.07965810081611077	0.978125
75	0.06756682855387529	0.9838541666666667	0.0808068976427118	0.9770833333333333
76	0.06400493315110604	0.984375	0.07937054373323918	0.978125
77	0.06325814587374529	0.9828125	0.07914831259598334	0.978125
78	0.05777439263959726	0.9846354166666667	0.07803720983987053	0.9802083333333333
79	0.06934369516869386	0.9809895833333333	0.078675335769852	0.978125
80	0.06558135350545248	0.984375	0.07801431013892095	0.978125
81	0.06464911463359992	0.9854166666666667	0.07818712356189887	0.9791666666666666
82	0.06584647378573814	0.98359375	0.07749663181602955	0.9802083333333333
83	0.06112940752257903	0.9859375	0.07760540945455432	0.9802083333333333
84	0.058385909907519816	0.9854166666666667	0.07769567519426346	0.9791666666666666
85	0.06039189472794533	0.98359375	0.08320683675507705	0.9770833333333333
86	0.05350020794818799	0.98671875	0.07837799427409967	0.978125
87	0.05763421803712845	0.9854166666666667	0.07893080872793992	0.978125
88	0.0585702088351051	0.9861979166666667	0.08024279040594896	0.978125
89	0.05614880894621213	0.9848958333333333	0.07866480462253093	0.978125
90	0.05669139251112938	0.9841145833333333	0.08009251238157351	0.978125
91	0.050817520109315714	0.9864583333333333	0.07623452258606751	0.9802083333333333
92	0.05199088944743077	0.9864583333333333	0.08017230816185475	0.978125
93	0.050695734781523547	0.9877604166666667	0.07707037593548496	0.9791666666666666
94	0.04976184219121933	0.9875	0.07608365314081311	0.9802083333333333
95	0.054384555046757065	0.984375	0.07689508606369297	0.9802083333333333
96	0.051756612956523895	0.9864583333333333	0.08323227129876613	0.9791666666666666
97	0.04721377907941739	0.98828125	0.07737835440784693	0.9802083333333333
98	0.05125965444991986	0.9875	0.07748119418198864	0.9802083333333333
99	0.050793493849535784	0.9869791666666666	0.07566585106154283	0.978125

The optimal condition:
	epoch: 98
	train_acc: 0.9875
	val_acc: 0.980208333333
	using time: 310.350090981
