The number of train datas: 6136
The number of test datas: 1536
epoch	train_loss	train_acc	val_loss	val_acc
0	0.7027677236200156	0.5003259457851793	0.6887100537618002	0.52734375
1	0.6952196078319127	0.5210234679407992	0.6839140107234319	0.572265625
2	0.6901614784106437	0.537157757185895	0.6804860681295395	0.5930989583333334
3	0.6882314774483403	0.5435136901663988	0.6775203744570414	0.6204427083333334
4	0.679890635427005	0.5695893087629544	0.6734877228736877	0.6432291666666666
5	0.674607429930253	0.5798565837053152	0.6675507724285126	0.6627604166666666
6	0.6694522191523884	0.5971316818774446	0.6607713103294373	0.6744791666666666
7	0.6622266219212458	0.6129400265418877	0.6520666380723318	0.703125
8	0.6582240137469504	0.6155475879275037	0.6411013702551523	0.7252604166666666
9	0.6454375244026234	0.6492829207802068	0.6267411112785339	0.751953125
10	0.6318882619075701	0.6659061280036688	0.6066671361525854	0.7858072916666666
11	0.612342330902776	0.6857887880276825	0.5829291939735413	0.8118489583333334
12	0.5905371790137409	0.7143089958555229	0.552209253112475	0.841796875
13	0.5633242014489553	0.7405475884714835	0.515425277252992	0.8743489583333334
14	0.524042032765907	0.77868318099242	0.4716612746318181	0.8938802083333334
15	0.48661027345539226	0.7962842243280374	0.4249869907895724	0.904296875
16	0.4401302602642363	0.8261082141309238	0.3770285149415334	0.9173177083333334
17	0.3946221910290799	0.8529986960636124	0.3310010681549708	0.9296875
18	0.3563232429257583	0.8763037812756436	0.29507752259572345	0.9322916666666666
19	0.3145243221653487	0.8955345505841241	0.27073004717628163	0.931640625
20	0.2800958062554898	0.9066166887849064	0.23739341522256532	0.9375
21	0.2652579084757565	0.91509126490067	0.22111602624257407	0.9459635416666666
22	0.24110835522529822	0.9207953067770838	0.21216284235318503	0.9498697916666666
23	0.23006646853848572	0.9279661021611837	0.20000542886555195	0.94921875
24	0.21594025732796382	0.9331812256318183	0.19077008465925852	0.9498697916666666
25	0.2066247955722324	0.9367666232850126	0.18702484791477522	0.9518229166666666
26	0.19703224946342826	0.9414928295932506	0.1816551505277554	0.953125
27	0.18453635496841417	0.9437744456599557	0.18152007708946863	0.953125
28	0.18256295119768015	0.9431225556438252	0.17415433501203856	0.953125
29	0.1752697698179404	0.9481747065715715	0.17146728498240313	0.9544270833333334
30	0.17312978598564824	0.9502933506393682	0.17189547481636205	0.9563802083333334
31	0.17011401732361145	0.9517601046133228	0.16643045097589493	0.955078125
32	0.16108034319952227	0.9543676661543616	0.16730687953531742	0.9583333333333334
33	0.15925680726494595	0.9553455015671144	0.16957745514810085	0.95703125
34	0.15591737519187132	0.9548565837053152	0.16240056666235128	0.9563802083333334
35	0.15182189542358204	0.9546936110847155	0.16058262810111046	0.95703125
36	0.15036107525909428	0.9581160359618872	0.16525018525620302	0.9596354166666666
37	0.1506911381748851	0.958767926599709	0.15811642011006674	0.95703125
38	0.1468578495250376	0.957790091730936	0.15754953461388746	0.9576822916666666
39	0.14486620653872384	0.9612125162195506	0.15769016618529955	0.9576822916666666
40	0.14735528319646668	0.9590938718409084	0.15561935926477113	0.9576822916666666
41	0.14397701787163722	0.9586049538236864	0.15772026342650255	0.958984375
42	0.14159796667689606	0.9621903519431491	0.1532386957357327	0.9576822916666666
43	0.13950302272141368	0.9630052154347047	0.15254292264580727	0.9602864583333334
44	0.13449933423601199	0.9643089965549255	0.15530669006208578	0.958984375
45	0.13395888503092054	0.9631681873559019	0.15249666136999926	0.9583333333333334
46	0.13319456330567325	0.9643089955446773	0.15201480748752752	0.9583333333333334
47	0.12540463300851676	0.9672425030263182	0.16652174852788448	0.9583333333333334
48	0.1286051142402236	0.9643089955446773	0.15174441070606312	0.958984375
49	0.12721405076312273	0.965775749907189	0.14908172624806562	0.9602864583333334
50	0.12949605150126417	0.9638200781491465	0.14956973058482012	0.958984375
51	0.12457794496957139	0.9669165584845213	0.1479025992254416	0.9596354166666666
52	0.12136087583127823	0.9664276397678967	0.14795775525271893	0.9596354166666666
53	0.12003791577035787	0.9685462838356933	0.1467573173965017	0.9596354166666666
54	0.12169907574275948	0.9682203394493193	0.149946765974164	0.9596354166666666
55	0.1211382377470177	0.9683833119144962	0.1457527900735537	0.9596354166666666
56	0.11936469216778817	0.9669165584845213	0.145545003314813	0.9596354166666666
57	0.11598857108508437	0.9698500646450684	0.14501432174195847	0.9602864583333334
58	0.11466247002646385	0.9701760101971135	0.144732930076619	0.9596354166666666
59	0.11489283146805372	0.9683833112150936	0.14374924575289091	0.9596354166666666
60	0.11250171622909809	0.9711538463869803	0.1447484064847231	0.958984375
61	0.11143533263787764	0.9701760105856705	0.14325135573744774	0.9602864583333334
62	0.11285924262608822	0.9706649284474698	0.1434642697374026	0.9596354166666666
63	0.10997089217119453	0.9701760108965162	0.14233280097444853	0.9602864583333334
64	0.1089705908537455	0.9718057369470907	0.1450761261706551	0.958984375
65	0.10758629767649176	0.9701760103525363	0.1416479144245386	0.9602864583333334
66	0.10750077770733926	0.9724576274294897	0.1413108790293336	0.9602864583333334
67	0.1072056124076694	0.9726205996615324	0.14092294064660868	0.958984375
68	0.10615868357433955	0.9721316823437129	0.14113255000362793	0.9602864583333334
69	0.1043460694818018	0.9737614083942876	0.1432098001241684	0.9596354166666666
70	0.10267381344811391	0.9724576274294897	0.1402816545839111	0.9609375
71	0.09940205489174794	0.9731095172124861	0.14135415262232223	0.9602864583333334
72	0.10136396497138321	0.9713168185413117	0.1403455094744762	0.9602864583333334
73	0.10265781496307841	0.9722946541094873	0.14149551124622425	0.958984375
74	0.09978828833206536	0.9726205998946666	0.14008011606832346	0.9602864583333334
75	0.10041365946364994	0.9716427644819138	0.13895825917522112	0.9602864583333334
76	0.09887179037777044	0.9731095177564658	0.14028395091493925	0.958984375
77	0.09745141030368158	0.9729465447473091	0.13939270159850517	0.9602864583333334
78	0.09694734301218931	0.9732724901439314	0.14012121626486382	0.9596354166666666
79	0.09856570748104732	0.9742503259452412	0.14301822676012912	0.9615885416666666
80	0.09335486900736487	0.9750651895145083	0.13901036450018486	0.958984375
81	0.0989649350690562	0.9731095177564658	0.13987450518955788	0.9596354166666666
82	0.08984722687609037	0.9752281618242624	0.13912901220222315	0.9609375
83	0.09609921637927382	0.9745762706424609	0.13796100206673145	0.9596354166666666
84	0.09108738064999064	0.9749022167384858	0.1383737384652098	0.9602864583333334
85	0.08809271588863263	0.9745762716527091	0.13900089729577303	0.9596354166666666
86	0.08924471889045746	0.9773468051926565	0.1395469388614098	0.9602864583333334
87	0.08829777123408958	0.9762059978587064	0.1396464705467224	0.9602864583333334
88	0.09085316744279676	0.9750651895145083	0.13847002666443586	0.9583333333333334
89	0.08833827261449927	0.9757170799969073	0.13821214872101942	0.9596354166666666
90	0.09181885772920183	0.9750651893590855	0.14027388549099365	0.9609375
91	0.08955081904396861	0.974739244117886	0.13811003975570202	0.958984375
92	0.08707031478432664	0.9744132990321094	0.1391470793945094	0.9615885416666666
93	0.08336286514880915	0.9766949147102575	0.1399249929624299	0.9602864583333334
94	0.08667712283686212	0.9755541073763075	0.13834182855983576	0.9596354166666666
95	0.08461885784681027	0.9755541073763075	0.1414068448357284	0.9635416666666666
96	0.08273317677489782	0.9763689703238834	0.13956500807156166	0.9596354166666666
97	0.08393287033114626	0.976531943099906	0.1392238736152649	0.958984375
98	0.08395877230859021	0.976531943099906	0.13817773511012396	0.9583333333333334
99	0.0821694692565223	0.977346805892059	0.1404481092467904	0.9576822916666666

The optimal condition:
	epoch: 95
	train_acc: 0.9755541073763075
	val_acc: 0.963541666667
	using time: 82.8432409763
