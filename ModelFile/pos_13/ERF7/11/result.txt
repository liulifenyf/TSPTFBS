The number of train datas: 10236
The number of test datas: 2560
epoch	train_loss	train_acc	val_loss	val_acc
0	0.690182822903137	0.5384915979311959	0.6357690125703812	0.746484375
1	0.5654779001109626	0.7596717468226687	0.4402853146195412	0.901953125
2	0.3573553249001177	0.8868698708104547	0.2649545304477215	0.90625
3	0.23293277023242945	0.9229191088639185	0.20624077282845973	0.92109375
4	0.18783888200115254	0.9366940208457791	0.18966588675975798	0.926171875
5	0.16613324330089707	0.9429464635787508	0.18421456683427095	0.930078125
6	0.15252450199658724	0.9467565453625508	0.1934675846248865	0.923046875
7	0.14320023636791845	0.9505666274258567	0.1744123550131917	0.933203125
8	0.13648319893653804	0.9543767099317139	0.1825387854129076	0.928515625
9	0.12532461125690428	0.956330598076138	0.17269519716501236	0.9328125
10	0.12344843793795789	0.9599452908731336	0.15961837470531465	0.9375
11	0.12019439896291334	0.9604337634682516	0.16204824345186353	0.937109375
12	0.11631753943473298	0.9618991793902324	0.15967305656522512	0.938671875
13	0.1140579385315361	0.9629738177345711	0.15743213538080453	0.9390625
14	0.11175870651109183	0.9628761233226915	0.1471675930544734	0.94453125
15	0.10682517219365412	0.9659046499977917	0.16228717621415853	0.940625
16	0.1056144158354072	0.9665885107877805	0.1515706479549408	0.944921875
17	0.10162377270719604	0.9678585386779356	0.14118816200643777	0.946875
18	0.0975578133513239	0.9680539275016948	0.1407910307869315	0.947265625
19	0.09787546633660864	0.9691285655898199	0.13334746491163968	0.948828125
20	0.09711678652802498	0.969812426566146	0.13551166392862796	0.949609375
21	0.09305581251751077	0.9722547872125195	0.1456200499087572	0.95
22	0.09093895517566233	0.9709847593223644	0.14563609659671783	0.95
23	0.09184295825016783	0.9727432593650862	0.150701043009758	0.9484375
24	0.08932565965250443	0.9731340363837164	0.15064460434950888	0.94765625
25	0.08776382602323261	0.9728409532412462	0.1307802801951766	0.951953125
26	0.08875147437719733	0.9721570926143025	0.13569775652140378	0.951953125
27	0.08695057091703867	0.9740132865331843	0.12640518695116043	0.95390625
28	0.08742905847783572	0.9731340363837164	0.13543891105800868	0.9515625
29	0.08384215433751759	0.9742086755432808	0.1372329535894096	0.951171875
30	0.08307209356908875	0.9747948419213901	0.13804118763655424	0.950390625
31	0.08104653256439799	0.9758694805452347	0.1340214941650629	0.952734375
32	0.08317344138146192	0.9730363419718367	0.1339287611655891	0.952734375
33	0.07991527434367135	0.9747948419213901	0.12847848343662918	0.9546875
34	0.08074249595333795	0.9754787022688278	0.12103578792884946	0.957421875
35	0.08161758974153766	0.9755763972164272	0.1307799884583801	0.9546875
36	0.08098988676674435	0.9757717860401864	0.12145481938496232	0.9578125
37	0.07699777338480568	0.9771395073639504	0.13281440194696187	0.955078125
38	0.07451884737642261	0.9776279793301799	0.13350542676635085	0.955078125
39	0.07653791395399244	0.9772372022183811	0.13104749317280948	0.95546875
40	0.07504913104889495	0.9768464246640312	0.12794460831210017	0.956640625
41	0.07333595335378737	0.978116451389578	0.12248569959774613	0.958203125
42	0.07321022244788278	0.977432591135309	0.13474821373820306	0.95390625
43	0.07339846899775893	0.9778233685964902	0.12053117584437131	0.958984375
44	0.07338334680849708	0.9785072290370966	0.12816027319058776	0.955859375
45	0.07466795288925993	0.9768464244776939	0.1335891698487103	0.953125
46	0.06921575333880885	0.9788003127152867	0.12183654732070863	0.95859375
47	0.07046059184518816	0.9788003122727356	0.12371140010654927	0.958203125
48	0.07233961874596562	0.9777256741846106	0.12474240018054843	0.9578125
49	0.06871486182683798	0.9795818680103238	0.12181414272636175	0.95859375
50	0.06767227298617597	0.9789957016322146	0.12657536594197155	0.95703125
51	0.07127848696131034	0.9782141463371775	0.1369848782196641	0.954296875
52	0.07039360224839625	0.978311840306506	0.12285774904303252	0.9578125
53	0.06702248663461362	0.9808518953647591	0.13342657862231136	0.95546875
54	0.06727654509604467	0.9798749514323	0.1274976076092571	0.9578125
55	0.06667378721245568	0.9804611173678581	0.12861937168054283	0.9578125
56	0.06569388332940862	0.9798749512459627	0.1311219995841384	0.956640625
57	0.06637807108604521	0.9796795625153721	0.13506532022729517	0.955078125
58	0.06385565373133803	0.9807542009528795	0.12623701808042825	0.95859375
59	0.06471300861736722	0.9810472837459673	0.13070186600089073	0.9578125
60	0.06437925389455067	0.9806565065409998	0.12253053202293814	0.960546875
61	0.06401850274265768	0.9802657289866499	0.13087973035871983	0.958203125
62	0.06205165462481785	0.9799726458441796	0.12037953184917569	0.96015625
63	0.06360646981590191	0.9814380619292056	0.12235130565240979	0.960546875
64	0.062092760127967994	0.9810472841885184	0.12576821390539408	0.95859375
65	0.06274330968728006	0.9815357562479166	0.12559808315709234	0.959375
66	0.06189308697506989	0.9809495896834701	0.12057342263869941	0.9609375
67	0.06244112980661563	0.9815357562479166	0.11955980276688934	0.9609375
68	0.060045077242379355	0.9818288394835555	0.1313407190144062	0.957421875
69	0.05978013441074333	0.982610394685424	0.1271767766214907	0.959375
70	0.05770981651572522	0.9822196173174114	0.13680024552159012	0.955078125
71	0.06042744852413233	0.9813403674241573	0.13210594025440514	0.9578125
72	0.059212465533617656	0.9822196172242428	0.12439661836251617	0.960546875
73	0.05766158335816352	0.9835873389905577	0.12478041425347328	0.960546875
74	0.058293770184946414	0.9813403674241573	0.115412163361907	0.962109375
75	0.056635853396630374	0.9813403674241573	0.1196446968242526	0.961328125
76	0.05832932674882467	0.9830011726124485	0.12964181173592806	0.958984375
77	0.058370421932818786	0.9821219229055318	0.12290926477871836	0.9609375
78	0.05704151797528703	0.9831965613430391	0.13024561158381404	0.958984375
79	0.05710718133088293	0.9830988667448222	0.12012773170135915	0.960546875
80	0.05543178033146275	0.9830988663954398	0.12616183944046497	0.96015625
81	0.055793712185655736	0.9838804222261966	0.11938409111462533	0.961328125
82	0.055592772768153116	0.983782727814317	0.1335865046828985	0.958203125
83	0.0567704702197236	0.9832942555685815	0.12286963146179915	0.960546875
84	0.05430675757559029	0.9834896443923408	0.132090734411031	0.95859375
85	0.05610573906546088	0.9827080893768095	0.12673876946792006	0.960546875
86	0.05392661990016268	0.9837827277211483	0.12391932192258537	0.960546875
87	0.05635841044219922	0.9839781165449076	0.12669333526864648	0.96015625
88	0.05140957277633567	0.9848573661586557	0.12686752914451063	0.960546875
89	0.05274646674348722	0.9845642831093541	0.1280521378852427	0.960546875
90	0.05302017880294322	0.9839781166380762	0.12108493857085705	0.9609375
91	0.05259985779530371	0.9851504495806319	0.1278148962184787	0.960546875
92	0.05238196638679542	0.984857366344993	0.12855204013176263	0.960546875
93	0.05051505946834114	0.9848573661586557	0.1244217989500612	0.9609375
94	0.0510613395540963	0.9849550607568727	0.12520680124871433	0.9609375
95	0.05097469654998401	0.9848573662518244	0.12007421762682498	0.9609375
96	0.04996710554953197	0.984857366344993	0.1259933383204043	0.960546875
97	0.04990784843983991	0.984661977428065	0.12409228496253491	0.96015625
98	0.05032273508781577	0.9845642829230168	0.12803615499287843	0.9609375
99	0.05077967214812614	0.9854435329094395	0.13445460894145073	0.957421875

The optimal condition:
	epoch: 74
	train_acc: 0.9813403674241573
	val_acc: 0.962109375
	using time: 648.104504824
