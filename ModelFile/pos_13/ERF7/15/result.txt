The number of train datas: 10236
The number of test datas: 2560
epoch	train_loss	train_acc	val_loss	val_acc
0	0.6913254754344855	0.5329230167102702	0.6346453219652176	0.754296875
1	0.5639207921791188	0.7576201644759945	0.4462418794631958	0.8890625
2	0.35423258469877134	0.8838413445779055	0.2755586005747318	0.904296875
3	0.23144815619358006	0.9218444704264112	0.22243728898465634	0.91328125
4	0.1925827228071495	0.9321023833942674	0.20612387582659722	0.920703125
5	0.17611457663106006	0.9362055490658869	0.2101082943379879	0.9171875
6	0.1640559149154139	0.9417741302635205	0.21941808387637138	0.9140625
7	0.15718591490403055	0.9451934347724765	0.20086580328643322	0.921484375
8	0.1521535027377677	0.9455842122336579	0.2098293885588646	0.91640625
9	0.14284453514254433	0.9496873776257714	0.202079869620502	0.920703125
10	0.1403777978065285	0.9520320435108828	0.1874606803059578	0.926953125
11	0.1380277404670093	0.9529112936603505	0.18745403327047824	0.92734375
12	0.13345002460731292	0.9527159047667149	0.1999458959326148	0.922265625
13	0.1303318166765397	0.9561352088098276	0.18832343351095915	0.92734375
14	0.12865979496088284	0.9545720984060907	0.17538617067039014	0.93046875
15	0.1252179784309403	0.9576006251743595	0.1888560994528234	0.927734375
16	0.1273645327257989	0.954865181991112	0.1752532647922635	0.930859375
17	0.1203247136252613	0.9589683471968883	0.17256380384787917	0.9328125
18	0.1182370285862775	0.9612153181344003	0.168165338691324	0.934375
19	0.11582128367895911	0.9624853454888356	0.15998902563005685	0.93828125
20	0.11408197086693114	0.9627784287244745	0.1558947847224772	0.9390625
21	0.1092196580789759	0.9639507616670302	0.17212561154738068	0.93671875
22	0.10792455137148645	0.9638530674414879	0.17593559259548783	0.93671875
23	0.1071251150527798	0.9643415398502685	0.1749497306533158	0.9359375
24	0.10493790682939497	0.964830011653453	0.1789981175214052	0.933984375
25	0.10205879711974361	0.9660023444096714	0.16222896771505474	0.941015625
26	0.1046473373101763	0.9660023448522224	0.16209410494193435	0.941796875
27	0.10146636970052965	0.9654161782877759	0.14771177247166634	0.943359375
28	0.1021823297899279	0.968444704799831	0.16180773647502064	0.940625
29	0.09656569508718485	0.9692262605374192	0.1683460739441216	0.9390625
30	0.09538639495211125	0.9692262599085308	0.16015597367659212	0.9421875
31	0.09608414413967185	0.9669792886216364	0.15629401211626828	0.943359375
32	0.09390207005052559	0.9694216488254588	0.15580115253105759	0.9453125
33	0.09264712227525167	0.9701055101511673	0.14702818244695665	0.94765625
34	0.09322707721873996	0.9700078152035679	0.14068718310445547	0.948828125
35	0.0916697166135135	0.9703985930374238	0.16232268577441572	0.944140625
36	0.0914700523443919	0.9714732314749311	0.1376100704073906	0.95078125
37	0.08721024823426361	0.9723524812750166	0.15440523717552423	0.94609375
38	0.08390980042974294	0.9724501755005589	0.15146374506875873	0.948046875
39	0.0852489844575996	0.9725478704481584	0.14592667850665747	0.949609375
40	0.08263415834513806	0.974794841478839	0.14332198100164534	0.949609375
41	0.081674134314363	0.9742086750075611	0.13581967437639833	0.953125
42	0.08336610388084983	0.973427119712524	0.1511634225025773	0.948828125
43	0.08086810188514267	0.9741109810382326	0.13610448250547053	0.952734375
44	0.08180593858477006	0.9744040637381518	0.14270097403787077	0.950390625
45	0.08070389862816843	0.9761625635945363	0.15180691187269985	0.9484375
46	0.07689481456562056	0.9763579524182957	0.14138635979034006	0.950390625
47	0.07802122532913768	0.9744040643670402	0.140731266932562	0.95078125
48	0.07890334324754743	0.9747948418282214	0.14062502305023372	0.95078125
49	0.07448019487503078	0.9764556468301753	0.13210006817243994	0.95390625
50	0.07547208157867694	0.9755763966807074	0.14639493227005004	0.951953125
51	0.07487871348654074	0.9760648691826568	0.14065134907141327	0.951171875
52	0.07493133094102063	0.9761625633383226	0.14101666053757073	0.9515625
53	0.07350305651997673	0.9783118407490571	0.13618558831512928	0.953125
54	0.07440306779671199	0.9778233687828276	0.13652345025911927	0.953125
55	0.07213660958177552	0.9784095347183857	0.14493184629827738	0.951953125
56	0.07074235693121059	0.9781164518321291	0.1504919584840536	0.95
57	0.07034412444499658	0.9775302855471887	0.14359927806071937	0.9515625
58	0.06746439208196867	0.978702618303407	0.136400459241122	0.955859375
59	0.06678924770409263	0.9792887846116397	0.15167200611904263	0.949609375
60	0.06803454620011759	0.9773348964439235	0.13644444141536952	0.956640625
61	0.06794728702076155	0.9784095352541055	0.14952175808139145	0.95078125
62	0.06569855993860352	0.9796795625153721	0.13443788262084128	0.957421875
63	0.06591726446341564	0.9801680344816016	0.1392479411326349	0.95546875
64	0.06517464924791494	0.9788980073135036	0.14814494729507716	0.95078125
65	0.06511192116849983	0.9799726455646737	0.13608933831565082	0.955859375
66	0.0631832924903301	0.980461117624072	0.13521296931430699	0.95625
67	0.06298913812645482	0.9806565067273372	0.1270759677514434	0.958984375
68	0.06170065685503997	0.9801680344816016	0.14999995343387126	0.951171875
69	0.060893768671755916	0.9814380617428683	0.14213944086804986	0.9546875
70	0.05968863851579664	0.9817311451648445	0.15774238468147814	0.948828125
71	0.06217399164859603	0.9812426730122776	0.14495207828003914	0.953125
72	0.06122124299331306	0.9801680346679389	0.13406031476333738	0.957421875
73	0.06056038509790857	0.9819265339886039	0.1458386060083285	0.953515625
74	0.06036199009298533	0.9807542010460482	0.12612899178639053	0.960546875
75	0.05762980470813057	0.9814380619292056	0.13585140667855738	0.9578125
76	0.057661939573944804	0.9832942557549188	0.14220509373117238	0.95390625
77	0.05789534692863973	0.9823173116361223	0.13888300163671374	0.9578125
78	0.055985988880936734	0.9830011719835601	0.14008407599758357	0.95703125
79	0.05453545441570786	0.9841735052754983	0.13188319015316666	0.9578125
80	0.056017841763820624	0.9818288396698929	0.13250554008409382	0.957421875
81	0.05631039118748733	0.9827080891904723	0.1369358512107283	0.957421875
82	0.05455915582405957	0.9835873389905577	0.14311866397038103	0.955859375
83	0.05588118241457163	0.9830988668379909	0.13764066461008043	0.9578125
84	0.052057734269527964	0.983880422133028	0.14821719131432473	0.952734375
85	0.05502125936741939	0.9822196167816917	0.14552335245534778	0.955859375
86	0.053605454663435616	0.9830988669311596	0.15089840285945683	0.951171875
87	0.05457155406620643	0.9836850334024373	0.15361589877866208	0.950390625
88	0.050460665285494144	0.9845642829230168	0.14192767266649753	0.956640625
89	0.050603450147486746	0.983587338897389	0.15163774217944592	0.9515625
90	0.052636348144902805	0.9835873388042204	0.13399011748842896	0.958203125
91	0.050446015997014974	0.9851504494874633	0.13256299174390734	0.95859375
92	0.049393964433498846	0.9851504493942946	0.14159385834354907	0.95859375
93	0.049084049358748515	0.9845642829230168	0.14186205863952636	0.95703125
94	0.04971636332289089	0.984857366344993	0.13277850451413542	0.960546875
95	0.048969404030578145	0.9856389214536928	0.13585307453759016	0.95859375
96	0.04577375544601234	0.9857366159587412	0.14996333043090998	0.9546875
97	0.0484713524494112	0.9849550605705354	0.1397831396199763	0.9578125
98	0.04812248312693851	0.9844665886043058	0.13556322981603444	0.96015625
99	0.04821898512492752	0.9861273935130911	0.15637791897170245	0.95078125

The optimal condition:
	epoch: 94
	train_acc: 0.984857366344993
	val_acc: 0.960546875
	using time: 698.468410969
