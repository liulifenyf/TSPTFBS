The number of train datas: 10236
The number of test datas: 2560
epoch	train_loss	train_acc	val_loss	val_acc
0	0.6925157071697194	0.5330207110522733	0.6389109045267105	0.76953125
1	0.5768594944579567	0.7517584995768787	0.45764061659574506	0.8890625
2	0.35636867358610935	0.8894099259618764	0.2690393730998039	0.9015625
3	0.2312419111256303	0.9222352481670983	0.22553233057260513	0.9109375
4	0.1920912413682822	0.9319069946636768	0.2130503762513399	0.912890625
5	0.17901948262071554	0.9358147713251997	0.21343844886869193	0.915625
6	0.17114943771937155	0.9392340754614812	0.22876496333628893	0.9078125
7	0.1641568651291112	0.9412856582972909	0.2061679668724537	0.918359375
8	0.1552456109702657	0.9459749905100647	0.21400489732623101	0.915234375
9	0.14778491576035166	0.9468542396812618	0.2166883995756507	0.913671875
10	0.14615115530325967	0.9494919888020121	0.1935680529102683	0.921875
11	0.14175297645397045	0.9508597110108782	0.19516108110547065	0.920703125
12	0.1378052842283957	0.9523251271191963	0.2052590443752706	0.915234375
13	0.1348859676046733	0.9537905428315476	0.19684388879686593	0.920703125
14	0.13348862245218882	0.9534974596890773	0.17679763864725828	0.930859375
15	0.12692936478694372	0.9589683466611686	0.1904685666784644	0.925
16	0.12858806557490426	0.9566236808692258	0.17468328326940535	0.9328125
17	0.12095323880175773	0.9595545136681661	0.17275807559490203	0.93359375
18	0.11953936501516407	0.9591637354849278	0.17185539249330758	0.933984375
19	0.1172299656540959	0.9593591245881931	0.1598909368738532	0.93671875
20	0.11544366867433259	0.9610199294969783	0.1566924812272191	0.9390625
21	0.10811140046001133	0.9648300120028354	0.177265562210232	0.932421875
22	0.1094121270612517	0.9641461505839581	0.17112383153289557	0.933984375
23	0.10580675375850558	0.9654161784741132	0.17629799265414475	0.9328125
24	0.10419468781989084	0.9650254004772122	0.17104283636435866	0.933984375
25	0.09860916397868771	0.9686400936235903	0.1566566487774253	0.940234375
26	0.10123955390127481	0.9659046505335114	0.15396254304796458	0.941796875
27	0.0973203279538027	0.969226260351082	0.1449554044753313	0.9453125
28	0.09898408015825265	0.9688354823541809	0.16180034102872015	0.940625
29	0.09426986639820631	0.9701055096154476	0.16201299633830785	0.940625
30	0.09142806818443101	0.9713755375056027	0.1624761413782835	0.941015625
31	0.09123973585970936	0.9693239545067478	0.1492407400161028	0.943359375
32	0.08919619618041201	0.9707893708712797	0.1494487000629306	0.943359375
33	0.08787969255113146	0.9720593984819288	0.14438550593331456	0.9453125
34	0.08990211402265855	0.970984759415533	0.1395969783887267	0.9484375
35	0.08931114364905998	0.9706916760867255	0.14922234304249288	0.94453125
36	0.08720021585134288	0.9714732314749311	0.1314149746671319	0.949609375
37	0.08490520647583924	0.9723524813681853	0.15334660611115397	0.9453125
38	0.08168025176801827	0.9744040639244891	0.14717742148786783	0.947265625
39	0.08191831717678885	0.973524814031235	0.14224600242450833	0.948046875
40	0.08051686877790211	0.9754787028977162	0.1447833544574678	0.949609375
41	0.07847671467853086	0.9731340370126048	0.13237132374197244	0.950390625
42	0.07885767372151099	0.9749902302094295	0.14977116631343962	0.949609375
43	0.07877472832932887	0.9757717859470179	0.13190637212246656	0.950390625
44	0.07796495538547012	0.9762602575638649	0.1418823966756463	0.9515625
45	0.08098970110200261	0.9741109809450639	0.14634118136018515	0.95078125
46	0.074253531249299	0.9775302855471887	0.13649630062282087	0.95078125
47	0.07574297088566308	0.9757717855044667	0.140799216972664	0.952734375
48	0.07515700736991848	0.9775302853608513	0.1415131526067853	0.951953125
49	0.07269548267510403	0.9769441189827421	0.1278054928407073	0.9515625
50	0.07295582878456865	0.9757717860401864	0.14199085063301026	0.951171875
51	0.07165936184097146	0.9787026184897444	0.13873478411696852	0.952734375
52	0.07147870988713526	0.9770418130452393	0.13774870736524464	0.952734375
53	0.07180242524302159	0.9773348966302607	0.13370076296851038	0.95234375
54	0.07099510285888345	0.9778233687828276	0.13209835011512042	0.952734375
55	0.07046408169248133	0.9772372018689987	0.14227078510448338	0.951171875
56	0.06775708119819852	0.9782141462440088	0.14640739588066937	0.95078125
57	0.0686110974452886	0.9778233687828276	0.14532186780124903	0.951171875
58	0.06533347961287939	0.9798749512459627	0.13352301623672247	0.952734375
59	0.06695133141646305	0.9783118406791806	0.1467057150322944	0.95078125
60	0.0650638812092388	0.9788980070339977	0.13385075558908283	0.95390625
61	0.06598781557311859	0.9771395078065014	0.14986734790727496	0.950390625
62	0.06343233162022606	0.9793864793729018	0.13225640999153257	0.953515625
63	0.06470437831638054	0.9798749513391314	0.13501035822555424	0.953125
64	0.06289866486882362	0.9805588122222888	0.1415553438477218	0.951953125
65	0.06465164624990175	0.9793864792797332	0.13856639987789093	0.953125
66	0.06140694189008766	0.9808518952715904	0.1388502400368452	0.953125
67	0.062359695352290745	0.9808518953647591	0.12320125643163919	0.95703125
68	0.06030590311071285	0.981535756434254	0.14770732773467898	0.95078125
69	0.05833253201683921	0.9820242282141461	0.14588735653087498	0.951171875
70	0.055948661309101005	0.9819265340817724	0.14572333600372075	0.951171875
71	0.06055340885420175	0.9809495896834701	0.14202711144462227	0.951953125
72	0.05854068934952388	0.9816334506597962	0.1371776602230966	0.953515625
73	0.05777815361654633	0.9833919501667984	0.14194008163176478	0.95234375
74	0.057567408266897636	0.9815357562479166	0.13200823487713934	0.9546875
75	0.05632655578692731	0.9824150059548333	0.13686783090233803	0.9546875
76	0.05686973504963774	0.9824150055122822	0.1439855174627155	0.9515625
77	0.056305765965898875	0.9830011719835601	0.13854104587808252	0.954296875
78	0.05568361593299007	0.9825127004598817	0.13907504663802683	0.952734375
79	0.055156324741522655	0.9829034781074002	0.1347085839137435	0.954296875
80	0.05499446744062138	0.9829034782005689	0.13417258942499757	0.953515625
81	0.05556040095407756	0.9831965611567018	0.13831396540626884	0.952734375
82	0.052540980655759656	0.9833919501667984	0.14385279081761837	0.95234375
83	0.055754232898372	0.982415006048002	0.13568528448231518	0.954296875
84	0.051806642200423664	0.9843688942855948	0.14746563220396638	0.951953125
85	0.05227359313257228	0.9830988663954398	0.13711063284426928	0.9546875
86	0.052191003234114394	0.9822196171310741	0.14482993260025978	0.952734375
87	0.053433460380333204	0.9841735054618356	0.1504512310028076	0.951171875
88	0.05018261421054031	0.9831965608073194	0.13561184117570518	0.95546875
89	0.05107099487911728	0.9831965612498705	0.14222001051530242	0.952734375
90	0.05075038909685011	0.9843688941924261	0.12919019106775523	0.9546875
91	0.04885344773213604	0.9848573662518244	0.13007854674942793	0.955078125
92	0.04741952194526192	0.9848573662518244	0.13466994548216463	0.955859375
93	0.04859838461566712	0.985052754982415	0.14480900957714765	0.95234375
94	0.04898416356923479	0.9842711997805466	0.14089618562720715	0.953125
95	0.046759593915216106	0.9851504494874633	0.13443407863378526	0.95546875
96	0.044795065270987935	0.9865181712537783	0.15511235361918807	0.95078125
97	0.04536871936852087	0.9857366161450785	0.1390462632291019	0.955078125
98	0.04784326048967497	0.983880422133028	0.12976581808179616	0.9546875
99	0.04529741166240305	0.9856389217331988	0.1622270295396447	0.950390625

The optimal condition:
	epoch: 67
	train_acc: 0.9808518953647591
	val_acc: 0.95703125
	using time: 760.288187027
