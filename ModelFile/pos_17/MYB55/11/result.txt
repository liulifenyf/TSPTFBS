The number of train datas: 4600
The number of test datas: 1152
epoch	train_loss	train_acc	val_loss	val_acc
0	0.706813763535541	0.4930434783126997	0.6928953925768534	0.5121527777777778
1	0.7006088275494783	0.5060869568327199	0.6906671126683553	0.5451388888888888
2	0.6931984810207201	0.5286956523812335	0.6890812714894613	0.5520833333333334
3	0.6918452534468278	0.5247826092139535	0.6877270009782579	0.5529513888888888
4	0.6849843241857446	0.5510869572473609	0.6861650082800124	0.5711805555555556
5	0.6860819338715595	0.5445652168730031	0.6841793523894416	0.5711805555555556
6	0.6844861634917881	0.5576086955485137	0.6819713976648119	0.5763888888888888
7	0.682239628771077	0.5586956518629322	0.6793181101481119	0.5946180555555556
8	0.6780933663119441	0.5734782603512639	0.6763440900378757	0.6059027777777778
9	0.6764998359265535	0.5741304353009099	0.6726411051220365	0.6232638888888888
10	0.6687812699442325	0.5928260868528615	0.6674194667074416	0.6432291666666666
11	0.6622582272861315	0.6095652169766633	0.6599915424982706	0.6579861111111112
12	0.6552312493324279	0.6184782615951869	0.6513012382719252	0.6675347222222222
13	0.6506693162088809	0.6226086954448534	0.6416318681504991	0.6753472222222222
14	0.6282414133652396	0.6673913039331851	0.6284039285447862	0.6996527777777778
15	0.623109667301178	0.667173913354459	0.6144475936889648	0.7213541666666666
16	0.6063597683284594	0.6871739127324975	0.5976455476548936	0.7109375
17	0.5880282993938611	0.7015217391304348	0.580159068107605	0.71875
18	0.573350223665652	0.713478261595187	0.5645581086476644	0.7222222222222222
19	0.5470586224224256	0.7354347819867342	0.546883655918969	0.7335069444444444
20	0.5324694731961126	0.7404347833343174	0.5319652524259355	0.7352430555555556
21	0.514327147214309	0.7565217385084733	0.5192883710066477	0.7369791666666666
22	0.5027305886538133	0.7636956519665925	0.508686770995458	0.7491319444444444
23	0.48824169713517895	0.7706521738093832	0.49908395608266193	0.7586805555555556
24	0.472673950402633	0.7849999993780384	0.48855021927091813	0.7664930555555556
25	0.46394077306208403	0.7819565214281496	0.4800226655271318	0.7708333333333334
26	0.4497915537979292	0.7958695659430131	0.47200294666820103	0.7699652777777778
27	0.4316731442057568	0.805	0.4585510624779595	0.7873263888888888
28	0.42109330669693323	0.8141304344716279	0.4473322133223216	0.7934027777777778
29	0.40798489492872486	0.8178260873711627	0.43633301390541923	0.8003472222222222
30	0.3998166152705317	0.8221739132507988	0.4246008296807607	0.8055555555555556
31	0.3802253753206004	0.8367391310567441	0.41185471084382796	0.8081597222222222
32	0.35755136572796364	0.8515217393377553	0.3992393778430091	0.8168402777777778
33	0.3528297415505285	0.8515217391304348	0.38477524783876205	0.8315972222222222
34	0.33482125914615135	0.8580434777425683	0.3707251052061717	0.8333333333333334
35	0.32583195152490035	0.8708695648027504	0.35766609840922886	0.8454861111111112
36	0.30619453917378964	0.8769565219464509	0.3462169071038564	0.8515625
37	0.2934099987537965	0.8880434781572093	0.3341226445304023	0.8602430555555556
38	0.27914679555789285	0.8932608700835186	0.3170364068614112	0.8689236111111112
39	0.26367755724036174	0.8960869563144186	0.3069603012667762	0.8784722222222222
40	0.253150844314824	0.9036956528995348	0.29414503773053485	0.8871527777777778
41	0.24213296483392302	0.9091304341606472	0.28288209438323975	0.8880208333333334
42	0.23570079559865206	0.9121739128361578	0.27099479403760696	0.8897569444444444
43	0.2232954936960469	0.9186956523812335	0.26249491506152683	0.8984375
44	0.21446627868258436	0.9191304341606472	0.2554260823461745	0.9010416666666666
45	0.20202022008273912	0.9256521744313447	0.2494166609313753	0.9045138888888888
46	0.1928142170543256	0.932826086645541	0.2393127746052212	0.9019097222222222
47	0.18869048253349635	0.9317391307457633	0.23333426151010725	0.90625
48	0.18087284772292428	0.9350000005183012	0.23295037117269304	0.9071180555555556
49	0.17877091907936593	0.9378260876821435	0.2235858572853936	0.9105902777777778
50	0.16750167993099793	0.9441304351972497	0.21969355642795563	0.9140625
51	0.1663714725556581	0.9430434779498889	0.21391014092498356	0.9149305555555556
52	0.15831451898035795	0.9443478259832963	0.20929725468158722	0.9184027777777778
53	0.15181494995303776	0.9454347827123559	0.20862981180349985	0.9175347222222222
54	0.1534687160409015	0.9452173910970273	0.201606841550933	0.9262152777777778
55	0.14240468007066975	0.9541304346789484	0.2012659708658854	0.9210069444444444
56	0.13813808492992236	0.9576086957558342	0.19626301527023315	0.9279513888888888
57	0.12987644892671835	0.9576086955485137	0.1946700563033422	0.9262152777777778
58	0.1251615793419921	0.9599999992743783	0.19262312683794233	0.9270833333333334
59	0.12746776902157328	0.957826086645541	0.19333128962251875	0.9244791666666666
60	0.12239982297886973	0.9593478253613348	0.18811034659544626	0.9305555555555556
61	0.12387259817641715	0.9567391310567441	0.18668556047810447	0.9314236111111112
62	0.12283999173537545	0.9573913036222044	0.18631392386224535	0.9305555555555556
63	0.1176542028396026	0.9634782613878665	0.18365197049246895	0.9357638888888888
64	0.11603371393421422	0.9600000006219616	0.18247467445002663	0.9322916666666666
65	0.11288849564998046	0.9626086962741355	0.18110853764745924	0.9392361111111112
66	0.11402734446784725	0.9626086949265521	0.1798603187004725	0.9340277777777778
67	0.10905122070208839	0.960434782297715	0.18000629792610803	0.9418402777777778
68	0.11196873178948527	0.9608695652173913	0.1819764706823561	0.9322916666666666
69	0.10281219553688298	0.9650000006219616	0.18723373694552314	0.9296875
70	0.10341573095839957	0.967391304762467	0.17574579434262383	0.9418402777777778
71	0.09807494311229042	0.9684782614915267	0.18273963199721444	0.9288194444444444
72	0.09721396044544552	0.9680434775352478	0.1794196003013187	0.9331597222222222
73	0.09547830900420314	0.9689130433746006	0.17734487934245002	0.9435763888888888
74	0.08922750539105871	0.9676086949265521	0.17609287715620464	0.9461805555555556
75	0.09251657676437627	0.9706521731874217	0.17305835005309847	0.9409722222222222
76	0.08777560024157814	0.9728260862309	0.17975357174873352	0.9340277777777778
77	0.09378715232662532	0.967826087060182	0.1785182919767168	0.9322916666666666
78	0.088278221708277	0.9717391308494236	0.1694774346219169	0.9409722222222222
79	0.08741277733574743	0.97086956469909	0.16923957731988695	0.9401041666666666
80	0.08562642131162726	0.9732608690469161	0.1690431675977177	0.9409722222222222
81	0.07969736963510514	0.9732608699798584	0.17142557435565525	0.9427083333333334
82	0.07916938741569933	0.9728260875784832	0.1702425264649921	0.9427083333333334
83	0.08404744492924732	0.9717391297091609	0.1703454554080963	0.9453125
84	0.07827430332484453	0.9717391310567441	0.1690384050210317	0.9444444444444444
85	0.07755502014056496	0.9734782610768857	0.17634940478536817	0.9383680555555556
86	0.08090341452671133	0.9741304354045702	0.17479291309913	0.9383680555555556
87	0.074307379839213	0.9771739134581192	0.1778940889570448	0.9392361111111112
88	0.07287794238847235	0.9776086962741354	0.173987478017807	0.9383680555555556
89	0.07102046885568163	0.9780434784681901	0.18004408478736877	0.9392361111111112
90	0.07761960452017577	0.9754347818830739	0.16968298455079397	0.9444444444444444
91	0.06761295817468477	0.9780434786755106	0.16902943038278156	0.9453125
92	0.06857217169974161	0.9760869569363801	0.17228620251019797	0.9427083333333334
93	0.06817338942185693	0.9763043480334075	0.1727296163638433	0.9427083333333334
94	0.06673602524011031	0.9786956523812336	0.17227856980429757	0.9418402777777778
95	0.06178399380134499	0.9799999992743782	0.17374671002229056	0.9427083333333334
96	0.06804243772574094	0.9782608697725379	0.17099026838938394	0.9453125
97	0.06859747833531836	0.9786956514482913	0.17430191487073898	0.9435763888888888
98	0.06259424190158429	0.9795652166656826	0.1733807474374771	0.9427083333333334
99	0.06287403424148974	0.9795652180132659	0.17193558315436044	0.9461805555555556

The optimal condition:
	epoch: 99
	train_acc: 0.9795652180132659
	val_acc: 0.946180555556
	using time: 299.965219975
