The number of train datas: 4600
The number of test datas: 1152
epoch	train_loss	train_acc	val_loss	val_acc
0	0.7094542020300161	0.49586956594301307	0.6941411230299208	0.5078125
1	0.699479510680489	0.5106521742240242	0.6899181074566312	0.5538194444444444
2	0.6937703070433243	0.5180434787791708	0.6869446171654595	0.5815972222222222
3	0.684939688599628	0.5552173912006876	0.683417472574446	0.5954861111111112
4	0.6819398873785267	0.5584782610768857	0.6798807316356235	0.6137152777777778
5	0.6830836510658265	0.557173913354459	0.6758883926603529	0.6232638888888888
6	0.6787323313174041	0.5704347827123559	0.6710495617654588	0.6414930555555556
7	0.6712789133320683	0.580869565113731	0.6642586125267876	0.65625
8	0.6656015540205914	0.6008695645954298	0.6546675231721666	0.671875
9	0.6551885899253513	0.624565216873003	0.6426119009653727	0.6866319444444444
10	0.64348346679107	0.642608695237533	0.6275649401876662	0.7109375
11	0.62706317766853	0.6652173919263094	0.612387067741818	0.6996527777777778
12	0.6108746169961017	0.6769565213244895	0.59225904279285	0.7222222222222222
13	0.5930606183798417	0.6910869570400404	0.5766199098693	0.7213541666666666
14	0.5669460327728935	0.7184782613878665	0.5604327321052551	0.7144097222222222
15	0.5595521425164264	0.7219565212208292	0.5504695011509789	0.7144097222222222
16	0.5423009828899218	0.7373913047624671	0.5392848286363814	0.7196180555555556
17	0.5250943876867709	0.7441304346789485	0.5313493609428406	0.7196180555555556
18	0.5171268873629362	0.7449999994816987	0.5258628593550788	0.7256944444444444
19	0.5012346301908078	0.7560869571437007	0.5189429819583893	0.7256944444444444
20	0.4889165952412978	0.7667391297091608	0.512605733341641	0.7309027777777778
21	0.48573622516963794	0.7684782606622447	0.5066706240177155	0.7378472222222222
22	0.473202159145604	0.7780434775352478	0.4980522692203522	0.7352430555555556
23	0.4656808890985406	0.7889130435819212	0.489772508541743	0.7430555555555556
24	0.45176682606987334	0.7882608688395957	0.47992109258969623	0.7482638888888888
25	0.44076295997785486	0.8017391303311223	0.4702899191114638	0.7638888888888888
26	0.4258135855716208	0.8089130430636199	0.4637467861175537	0.7708333333333334
27	0.41486886418384056	0.8173913043478261	0.44815640648206073	0.78125
28	0.4073307890477388	0.8221739136654398	0.43764201800028485	0.7907986111111112
29	0.3864955279101496	0.8339130428562994	0.4234798219468858	0.7994791666666666
30	0.3791301686349122	0.8393478265015976	0.4124068452252282	0.8055555555555556
31	0.36133004307746885	0.8489130438929019	0.40063730875651044	0.8125
32	0.3419698827681334	0.8632608700835187	0.3897250493367513	0.8151041666666666
33	0.3345644535707391	0.8656521744313448	0.3731456200281779	0.8307291666666666
34	0.3180568410520968	0.8730434784681901	0.3608860241042243	0.8350694444444444
35	0.3117699895216071	0.8784782602476037	0.3479902810520596	0.8446180555555556
36	0.29844998499621517	0.8799999992743782	0.34003736906581455	0.84375
37	0.2862758738061656	0.8819565224647522	0.33430494864781696	0.8498263888888888
38	0.27294336075368136	0.8939130439965621	0.3172909981674618	0.8628472222222222
39	0.26266266092010165	0.9002173908897069	0.3096017407046424	0.8697916666666666
40	0.24871679176454958	0.9052173913043479	0.3018946879439884	0.8723958333333334
41	0.24005827159985252	0.9150000005183012	0.29401709470483994	0.8802083333333334
42	0.2324805122354756	0.9110869570400404	0.2846831762128406	0.8897569444444444
43	0.22934927904087565	0.9132608699798584	0.27832653952969444	0.8888888888888888
44	0.2128330945191176	0.9195652171839839	0.2728356271982193	0.8932291666666666
45	0.21295674723127614	0.9219565213244895	0.26871295107735527	0.8932291666666666
46	0.19902110307112983	0.9249999998963397	0.2631424119075139	0.8940972222222222
47	0.1942925848390745	0.9282608702908391	0.2610103421741062	0.8975694444444444
48	0.1964842640057854	0.9295652170803236	0.25894445677598316	0.9010416666666666
49	0.1867576498311499	0.9350000001036602	0.25170116126537323	0.8993055555555556
50	0.18082465837831083	0.9349999994816988	0.247010984354549	0.9019097222222222
51	0.1800795467003532	0.9363043476187665	0.24962925414244333	0.9045138888888888
52	0.17296689582907634	0.9356521742240242	0.24018451074759165	0.9036458333333334
53	0.16776336402996728	0.9373913044514863	0.24379152059555054	0.9045138888888888
54	0.1665250593682994	0.9432608693578969	0.23500761886437735	0.9053819444444444
55	0.16313284956890603	0.9439130437892417	0.2412947201066547	0.9071180555555556
56	0.15343986487906913	0.9443478253613348	0.23140203124947017	0.9079861111111112
57	0.15246449206186377	0.9480434781572094	0.22961930930614471	0.9114583333333334
58	0.14806013353492903	0.9443478266052577	0.22756820420424143	0.9140625
59	0.14462986715461898	0.9504347820903944	0.2262876464260949	0.9131944444444444
60	0.14416783817436385	0.9489130437892416	0.22607393893930647	0.9175347222222222
61	0.13894203991993614	0.9541304342643074	0.22373436060216692	0.9105902777777778
62	0.13492319651271986	0.9554347832306571	0.22467777298556435	0.9166666666666666
63	0.13398463371007338	0.9541304353009099	0.22082659933302137	0.9175347222222222
64	0.13066049647072087	0.9560869562107583	0.2196540269586775	0.9166666666666666
65	0.12356819568768791	0.9593478255686553	0.21921036806371477	0.9192708333333334
66	0.1273374778032303	0.9584782608695652	0.21633260117636788	0.9175347222222222
67	0.12448541274537211	0.9599999992743783	0.21484165224764082	0.9192708333333334
68	0.12060351380835409	0.9584782603512639	0.21929936442110273	0.9184027777777778
69	0.11464083865932796	0.9589130427526391	0.21922288007206386	0.9184027777777778
70	0.11335212137388147	0.963695652588554	0.2123054580556022	0.9227430555555556
71	0.11044454289519269	0.9626086960668149	0.22595904684729046	0.9184027777777778
72	0.11106383862702743	0.963043478882831	0.2116300861040751	0.9192708333333334
73	0.10988627669603929	0.9645652174949646	0.20932316117816502	0.9236111111111112
74	0.10389062059962231	0.9667391308494236	0.21199400226275125	0.921875
75	0.10641602431950362	0.9667391310567441	0.20901028480794695	0.9253472222222222
76	0.09822534846222919	0.9678260862308999	0.21064471536212498	0.921875
77	0.09668246851019237	0.9686956516556118	0.21615628235869938	0.9184027777777778
78	0.09452142277489538	0.9697826093176137	0.2097135086854299	0.9210069444444444
79	0.09298895778863327	0.9706521731874217	0.21075262294875252	0.9184027777777778
80	0.09658824700376262	0.9658695658393528	0.2064763688378864	0.9236111111111112
81	0.09144515014213064	0.9721739136654398	0.2081136273013221	0.9253472222222222
82	0.08345421053793119	0.9743478265015976	0.20708930989106497	0.9270833333333334
83	0.09249771688295447	0.968043478882831	0.20702923585971197	0.9253472222222222
84	0.08791390542102896	0.9695652166656826	0.20518104400899675	0.9244791666666666
85	0.08787271389494772	0.9704347818830739	0.2051698880063163	0.9253472222222222
86	0.09011007569406343	0.9676086962741354	0.2031683772802353	0.9253472222222222
87	0.08021029094639032	0.9728260873711627	0.20603815383381313	0.9244791666666666
88	0.08028155869763831	0.9756521731874217	0.2051550911532508	0.9262152777777778
89	0.07464692593916603	0.9754347832306571	0.2087799393468433	0.9296875
90	0.08132006839565609	0.9730434775352478	0.2023729392223888	0.9262152777777778
91	0.07637901418882867	0.9750000002073205	0.2031333347161611	0.9253472222222222
92	0.0775002165980961	0.9750000006219615	0.2039518720573849	0.9244791666666666
93	0.07111762192586193	0.976304348240728	0.20373859339290196	0.9244791666666666
94	0.07284258767314579	0.9767391297091609	0.20800720155239105	0.9279513888888888
95	0.06982411930094594	0.9758695658393528	0.2126266443067127	0.9296875
96	0.07026601377388705	0.9778260873711627	0.20460133420096505	0.9253472222222222
97	0.06826371065948321	0.9795652168730031	0.2040613293647766	0.9270833333333334
98	0.06557712759012761	0.9789130438929019	0.21355929474035898	0.9253472222222222
99	0.06477753793415816	0.9793478253613348	0.20666609042220646	0.9288194444444444

The optimal condition:
	epoch: 95
	train_acc: 0.9758695658393528
	val_acc: 0.9296875
	using time: 452.160881996
