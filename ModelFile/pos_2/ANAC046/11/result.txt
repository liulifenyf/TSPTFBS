The number of train datas: 6660
The number of test datas: 1666
epoch	train_loss	train_acc	val_loss	val_acc
0	0.7050373787636514	0.49624624624624625	0.696885500778528	0.5090036014405762
1	0.6991140604377151	0.5054054054054054	0.6922809363556366	0.5084033613445378
2	0.6932923068871369	0.5213213213213214	0.6896524922329695	0.5264105642256903
3	0.6906392630872068	0.5243243243243243	0.6873820550730821	0.5426170468187275
4	0.6874226566549536	0.5402402402402402	0.6847155992867423	0.5576230492196879
5	0.6835978898558173	0.5551051051051051	0.6816527922900498	0.5690276110444178
6	0.682710466227374	0.5539039039039039	0.6780871105652039	0.5930372148859544
7	0.6758102215088165	0.5732732732732733	0.6732344907157275	0.6032412965186075
8	0.6735257707200609	0.5863363363363363	0.6672171454469696	0.6290516206482593
9	0.6631315953022725	0.6079579579579579	0.659977472772976	0.641656662665066
10	0.6588556412104014	0.6186186186186187	0.6507974344999994	0.6674669867947179
11	0.6486660546368664	0.6303303303303304	0.6404757757289927	0.6848739495798319
12	0.6404260807209187	0.642942942942943	0.6300396843403041	0.6992797118847539
13	0.6314079220230515	0.6578078078078078	0.6178118555771918	0.6968787515006002
14	0.6117646970548429	0.6792792792792792	0.6004603341275475	0.7244897959183674
15	0.5998295390212142	0.6887387387387387	0.5825144275754582	0.7262905162064826
16	0.5834102236114823	0.7043543543543543	0.5663876098816564	0.7328931572629052
17	0.5600120929984359	0.721921921921922	0.5413570854844165	0.7617046818727491
18	0.5407487040346449	0.7364864864864865	0.519545936999487	0.7785114045618248
19	0.5171076836528721	0.7612612612612613	0.5101004795295422	0.7533013205282113
20	0.49630514966832984	0.7656156156156156	0.4850528665593549	0.7719087635054022
21	0.4731444778743091	0.7845345345345346	0.45533684454187484	0.8025210084033614
22	0.45688034131362276	0.7965465465465466	0.4311060808142837	0.8229291716686674
23	0.4296470976090646	0.8165165165165165	0.40415975995281306	0.8349339735894358
24	0.4095965631015308	0.827027027027027	0.3911227000002958	0.843937575030012
25	0.3893733851186506	0.8432432432432433	0.3659331548113783	0.8505402160864346
26	0.3642668216436117	0.8507507507507508	0.36060186608785055	0.8547418967587035
27	0.3530375038181339	0.8579579579579579	0.33214519204211834	0.8631452581032413
28	0.3359699143840744	0.8713213213213213	0.31441971910100025	0.8703481392557023
29	0.32825791381142877	0.8746246246246246	0.305118601934678	0.8751500600240096
30	0.31020197531840465	0.8836336336336337	0.2960094895039429	0.8835534213685474
31	0.2989279795426864	0.8897897897897898	0.2888660947887265	0.8853541416566627
32	0.28765183357863094	0.8927927927927928	0.28151143962094766	0.8865546218487395
33	0.2798044431853939	0.8972972972972973	0.2719076853446743	0.8883553421368547
34	0.2738290727764994	0.8986486486486487	0.27006491127849913	0.8889555822328932
35	0.26452045888156145	0.9069069069069069	0.2662484952453233	0.8961584633853541
36	0.2532889293478774	0.9109609609609609	0.2562257089391619	0.9003601440576231
37	0.2505819667567004	0.909009009009009	0.2332623271452708	0.9063625450180072
38	0.240871336904016	0.9135135135135135	0.22871518585862233	0.9087635054021609
39	0.24225185174752284	0.9135135135135135	0.23735074983352944	0.9099639855942377
40	0.23638826096738064	0.9202702702702703	0.21838312414513916	0.9111644657863145
41	0.22670442003745575	0.9232732732732732	0.22522605159560313	0.9135654261704682
42	0.2240704403759481	0.9238738738738739	0.2103924710138076	0.9195678271308524
43	0.21869696270640906	0.927027027027027	0.2059622665270179	0.918967587034814
44	0.21055336564152807	0.9304804804804805	0.24102577703053496	0.9069627851140456
45	0.2050575736406687	0.9321321321321321	0.19842931098726188	0.9267707082833133
46	0.20540369634513742	0.9325825825825825	0.1954170121114795	0.9273709483793517
47	0.20182439728944868	0.9325825825825825	0.19946672578676553	0.9255702280912365
48	0.1943700943981205	0.9366366366366367	0.19659254131984025	0.9267707082833133
49	0.1987141943595431	0.9351351351351351	0.19639202043646667	0.9261704681872749
50	0.19319983926621284	0.9373873873873874	0.18455683870189615	0.9327731092436975
51	0.18521590612314126	0.9391891891891891	0.18468439101385756	0.9345738295318127
52	0.18413902739504795	0.9397897897897898	0.18250269509878767	0.9351740696278511
53	0.1897432301271785	0.940990990990991	0.19204989495147176	0.9297719087635054
54	0.1743538150826732	0.943993993993994	0.17843229515927464	0.9351740696278511
55	0.17610463708102167	0.9457957957957958	0.20160615252775876	0.9267707082833133
56	0.17378104692464835	0.9454954954954955	0.18390974807603305	0.9333733493397359
57	0.1718185798004941	0.9448948948948949	0.17729616787682634	0.9357743097238895
58	0.16716104816119592	0.9477477477477477	0.17105108527254229	0.9417767106842737
59	0.16188464220669177	0.9472972972972973	0.17729789608953095	0.936374549819928
60	0.15957407816215322	0.9503003003003003	0.17095884566261274	0.936374549819928
61	0.16641582850221398	0.946996996996997	0.17015215473658754	0.9411764705882353
62	0.1607389928789826	0.9496996996996997	0.17637682078229566	0.9369747899159664
63	0.1566070654281267	0.9528528528528528	0.16591155252644138	0.9423769507803121
64	0.1599910400472246	0.9503003003003003	0.1785349971642729	0.936374549819928
65	0.15493226072809718	0.9519519519519519	0.1641845064146035	0.9399759903961584
66	0.15807369902349627	0.9506006006006006	0.16452828670392375	0.9423769507803121
67	0.1474548958532803	0.9566066066066066	0.1719122203100486	0.9399759903961584
68	0.14969007823470834	0.9548048048048048	0.16909582485385588	0.9405762304921969
69	0.1444322161555469	0.9554054054054054	0.17334313368668505	0.93937575030012
70	0.14713426817980435	0.9554054054054054	0.1696946756268034	0.9417767106842737
71	0.1465393525836346	0.9554054054054054	0.17293966780690587	0.9411764705882353
72	0.14197660895797226	0.9533033033033033	0.16186443107182524	0.9405762304921969
73	0.14237656016965528	0.9540540540540541	0.16900959841701307	0.9423769507803121
74	0.1447918716180432	0.9543543543543543	0.1618514311771576	0.943577430972389
75	0.14515249696937768	0.9572072072072072	0.1862216561424489	0.9333733493397359
76	0.1487043181965659	0.9551051051051052	0.16594039189930007	0.9447779111644657
77	0.1369373574002727	0.9576576576576576	0.17845731249161842	0.9387755102040817
78	0.1376481562987104	0.9575075075075075	0.16255866103813427	0.9459783913565426
79	0.13430129525450257	0.9605105105105105	0.16928248842652677	0.9429771908763506
80	0.13408577610981895	0.9599099099099099	0.1624742902877952	0.9447779111644657
81	0.13249126522390692	0.9597597597597598	0.19521087007809515	0.9297719087635054
82	0.1436013212343594	0.9573573573573574	0.17265842581281857	0.9405762304921969
83	0.13425725633586133	0.9588588588588589	0.16482405666829872	0.9477791116446579
84	0.12993795119964324	0.959009009009009	0.1622210622799783	0.9471788715486195
85	0.1275795206248581	0.9611111111111111	0.17122218169334555	0.9423769507803121
86	0.12710558370247976	0.9603603603603603	0.1700713521435338	0.9429771908763506
87	0.12942557412761826	0.9599099099099099	0.1583878960715336	0.9441776710684273
88	0.12485959533456567	0.9629129129129129	0.1641647434678255	0.9399759903961584
89	0.1357303940072969	0.9561561561561561	0.1623992515998442	0.9441776710684273
90	0.12447173527243349	0.9627627627627627	0.17346362277668637	0.9423769507803121
91	0.12096345379456386	0.9642642642642643	0.15825903313715203	0.9459783913565426
92	0.12148503253946791	0.9629129129129129	0.15943335656787738	0.9447779111644657
93	0.1197220518558591	0.9615615615615616	0.1686053490277384	0.943577430972389
94	0.11742846828440691	0.9657657657657658	0.15837175262217618	0.9459783913565426
95	0.11548935890326405	0.9657657657657658	0.16193684890848392	0.943577430972389
96	0.11791708536065734	0.9632132132132132	0.1594589630327448	0.9447779111644657
97	0.11628801597578747	0.9665165165165165	0.1597343613298572	0.9453781512605042
98	0.11181135270688627	0.965015015015015	0.18437040861950918	0.9357743097238895
99	0.12074887587859466	0.963963963963964	0.16045383192232104	0.9459783913565426

The optimal condition:
	epoch: 83
	train_acc: 0.9588588588588589
	val_acc: 0.947779111645
	using time: 427.584255934
