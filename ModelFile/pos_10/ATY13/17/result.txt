The number of train datas: 10000
The number of test datas: 2502
epoch	train_loss	train_acc	val_loss	val_acc
0	0.7041912475585937	0.5178	0.6879320916988486	0.5459632294879352
1	0.6924587381362916	0.5264	0.6822463516994632	0.5799360511828956
2	0.6862668830871582	0.5493	0.6762394444357387	0.6115107909380961
3	0.6801388389587403	0.5628	0.665941228921846	0.649880096447363
4	0.6708899101257324	0.5922	0.6515979071696408	0.6686650682315171
5	0.6561720602035522	0.6148	0.6304604565020469	0.6918465230200026
6	0.6365930654525757	0.6408	0.6022626348346066	0.7158273376053949
7	0.6154737034797668	0.6616	0.5768870912867484	0.7202238203714982
8	0.5943361916542054	0.6856	0.5498143954338025	0.7370103911625491
9	0.5780428519248962	0.6939	0.5407471707065423	0.7254196636491923
10	0.5655386693477631	0.7088	0.5196755285933912	0.7573940842557583
11	0.54969359664917	0.7141	0.5047219789666618	0.7685851322756492
12	0.5423457620620727	0.7261	0.5125675548037751	0.7406075133217705
13	0.5349701594352723	0.7323	0.4814394566652586	0.7801758596937147
14	0.5145160584449768	0.7503	0.46859283791743306	0.7861710625777332
15	0.5025739054679871	0.7531	0.4683389854278686	0.7833733015018497
16	0.48545932998657226	0.77	0.436144577727901	0.8101518787830758
17	0.4696618521690369	0.7804	0.41846530533713594	0.8225419667127322
18	0.44131576175689696	0.8029	0.40158838388635865	0.8337330129697359
19	0.43003363709449766	0.8081	0.3906111877551563	0.8393285365032254
20	0.41005457701683046	0.8187	0.3649983968761423	0.8549160664792446
21	0.38633340764045715	0.841	0.3478096638509124	0.8637090329167177
22	0.37508701915740966	0.8441	0.33087824984229536	0.8713029569192089
23	0.3613436866760254	0.8552	0.31476376737526757	0.8752997603347833
24	0.3525796031475067	0.857	0.30496599855754586	0.8808952839159184
25	0.33499366149902343	0.8678	0.30086967508665186	0.8840927259146357
26	0.3214004671096802	0.8728	0.2914379601188891	0.8880895284725894
27	0.31307340650558474	0.8805	0.27730803915874946	0.8956834525227261
28	0.30851744630336764	0.8814	0.28134376160818325	0.8968824940047961
29	0.29716563694477083	0.8881	0.285015659211732	0.8892885691446842
30	0.29423466925621033	0.8862	0.2862828117575672	0.885691446842526
31	0.28561215407848356	0.8946	0.25237814288773985	0.9052757786618149
32	0.27768028240203857	0.8951	0.25343707515943725	0.9088729008686819
33	0.27405012760162356	0.8951	0.2884557484675178	0.8828936850519584
34	0.2670604676961899	0.9029	0.2544616604987666	0.9092725819344525
35	0.25852447617053986	0.903	0.25459633104139856	0.9072741806554756
36	0.25444863052368166	0.9038	0.23981187384810856	0.9084732207081777
37	0.2532410476565361	0.9074	0.23478449848916033	0.9132693836824309
38	0.2482560626268387	0.9084	0.23923824205220365	0.913269384492406
39	0.24599429483413696	0.911	0.2315569956549447	0.9140687441940216
40	0.23934324507713317	0.9142	0.22808472407188157	0.9164668257287938
41	0.23469872722625731	0.9139	0.22560300658360946	0.9184652270077707
42	0.23361906461715698	0.9155	0.24412314983294736	0.907673860911271
43	0.23108824217319487	0.9164	0.2217180756546801	0.9168665059845892
44	0.22633267827033995	0.9172	0.2441024663184377	0.9064748201438849
45	0.22138433723449708	0.9199	0.22207346264954855	0.9176658673061551
46	0.21913920187950134	0.9217	0.21819572793327266	0.9188649080735412
47	0.21930381722450257	0.9197	0.2349607837524155	0.9096722621902478
48	0.21210428855419158	0.9231	0.21632487711003073	0.921262989655959
49	0.20783977822065353	0.9238	0.22024348697406973	0.9176658673061551
50	0.20776170649528503	0.9263	0.21086961447859076	0.9220623501199041
51	0.20077917861938477	0.9236	0.21017279847563028	0.9224620303756994
52	0.20206242790222168	0.9263	0.2127323763476287	0.9192645875670069
53	0.19858552011251449	0.9298	0.20786090962868706	0.9220623501199041
54	0.19905102450847625	0.9303	0.21121810559484122	0.9204636290967226
55	0.19325460443496703	0.9336	0.2116400692388594	0.9216626698641087
56	0.19226463599205018	0.9312	0.20903689280259524	0.9208633094001636
57	0.18795250012874604	0.9325	0.20585931813140376	0.9224620303756994
58	0.190040562748909	0.9328	0.20553368589670348	0.9228617106791404
59	0.18465802265405654	0.9302	0.21405967685292	0.920863309352518
60	0.18741253974437713	0.9348	0.21770984858429307	0.9180655475619505
61	0.18265426769256593	0.9369	0.20298025168055633	0.9244604308447011
62	0.17712306218147278	0.9363	0.20501942216730612	0.9224620303756994
63	0.1761262879371643	0.9372	0.20266379908525306	0.923261390077315
64	0.1758046149492264	0.9388	0.21610322152729705	0.9180655475619505
65	0.17325507088899614	0.9368	0.20480615997628915	0.920863309352518
66	0.17096348168849945	0.9398	0.20927057079345487	0.9188649080735412
67	0.1722960364818573	0.9396	0.20361650695236658	0.920863309352518
68	0.16828646941184996	0.9403	0.2016256544515669	0.9220623493099289
69	0.17016998770236968	0.9405	0.21609792414662554	0.9188649080735412
70	0.16462505541443825	0.9412	0.20726743041516113	0.9232613901249606
71	0.16506332342624663	0.9402	0.21414694573870188	0.919664268585132
72	0.16310847928524017	0.9442	0.22009073044780156	0.9172661870503597
73	0.15951095228195192	0.9425	0.2012854735437724	0.9244604308447011
74	0.15829766492247582	0.9424	0.20146888726168305	0.9220623493099289
75	0.15873352065086366	0.9431	0.2009940968154908	0.9248601111004965
76	0.15544958457946778	0.9463	0.20148351642010595	0.923261390077315
77	0.15740140886306764	0.9436	0.20054291518067094	0.923661070380756
78	0.15694163641929626	0.9455	0.2047306920746438	0.9240607506365514
79	0.15185382804870606	0.9447	0.2018731931249777	0.9208633085425428
80	0.15084080274105072	0.9463	0.20152908943587547	0.9216626690541335
81	0.1487817687690258	0.9453	0.2023059771501189	0.9220623493099289
82	0.14443841507434846	0.9503	0.20383088818366388	0.9216626690541335
83	0.14297959095835686	0.9503	0.20873160894921453	0.9216626698641087
84	0.1410317093372345	0.9499	0.20756651397398432	0.9220623501199041
85	0.14140191240310668	0.9507	0.2040777664831121	0.9208633085425428
86	0.1409720862388611	0.9532	0.20510594327029563	0.923661070380756
87	0.14136415672302247	0.9517	0.2039509029756728	0.9220623502151952
88	0.1389726572751999	0.9516	0.21285995901178875	0.9192645883293366
89	0.13732995529174805	0.9521	0.21227933061446885	0.9180655475619505
90	0.13759950442314148	0.9517	0.2039738760220347	0.9212629887983381
91	0.13257390110492706	0.9548	0.2099199199663411	0.9184652278177458
92	0.13348972020745278	0.9525	0.20799444581297852	0.9192645875193614
93	0.12923766479492188	0.9552	0.22030911431740896	0.9180655475619505
94	0.1306317422389984	0.9526	0.20560360416996298	0.922861710726786
95	0.13361563091278075	0.9539	0.22852885492032857	0.9148681055155875
96	0.12540529403686523	0.958	0.22168462222619212	0.9180655475619505
97	0.12786016633510588	0.9557	0.20896527608878893	0.9204636282867474
98	0.1254470421552658	0.9545	0.20966815448803106	0.9192645884246278
99	0.12364971890449523	0.9579	0.20801727210493873	0.9216626699117543

The optimal condition:
	epoch: 75
	train_acc: 0.9431
	val_acc: 0.9248601111
	using time: 621.925275803
