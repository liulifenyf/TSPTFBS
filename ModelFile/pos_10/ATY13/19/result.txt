The number of train datas: 10000
The number of test datas: 2502
epoch	train_loss	train_acc	val_loss	val_acc
0	0.7032521284103393	0.5035	0.6874357028354368	0.5451638689048761
1	0.691558622455597	0.5284	0.6819450063385266	0.588329336697535
2	0.6848524981498718	0.5497	0.6754383234668979	0.6171063152815608
3	0.677152099609375	0.5672	0.6634358540236902	0.6522781770768687
4	0.6653129856109619	0.5976	0.6468161218743816	0.6814548364169687
5	0.6508992722511292	0.6254	0.6220554130540478	0.7002398084393509
6	0.6280631672859192	0.6534	0.5918087077845963	0.7182254200454333
7	0.6066516576766968	0.6675	0.5721277085711344	0.71662669806934
8	0.5914204235076904	0.6854	0.5488266881993062	0.7258193440479245
9	0.5763016813278198	0.6938	0.5399695641274075	0.7330135885569499
10	0.5649785845756531	0.7076	0.5214367706379254	0.7470023976050788
11	0.5497759517669678	0.7184	0.5060302786928096	0.75779376541682
12	0.5424197484016419	0.7235	0.5126626695469796	0.7430055957141636
13	0.5336307826042175	0.7306	0.4836766952090412	0.7789768181163534
14	0.5169671357154846	0.7482	0.47382581905781224	0.779776178532653
15	0.505189380455017	0.7545	0.474120154536123	0.7705835324587772
16	0.48630356216430665	0.7709	0.4398935283783624	0.805755395111706
17	0.4697713871002197	0.7788	0.4208742448037191	0.8205435653384641
18	0.4408561071395874	0.799	0.40212315845070223	0.8249400481045674
19	0.429007926940918	0.8105	0.39268082713813995	0.826139088824308
20	0.4075819543838501	0.8248	0.36770953131998946	0.8453237402925102
21	0.384596804523468	0.841	0.35140061460667665	0.8537170264741881
22	0.3710365636348724	0.846	0.3312906264353523	0.8741007195197517
23	0.3557234094619751	0.858	0.31801154072002635	0.878896882589296
24	0.34471849839687346	0.8632	0.30763374466737875	0.8808952830582977
25	0.3276698098182678	0.8689	0.29686915105004774	0.8856914469378172
26	0.315360639667511	0.8744	0.294129775296584	0.8868904876575576
27	0.305282560634613	0.8822	0.2792599602854795	0.8924860104287176
28	0.3010162542819977	0.882	0.298552737146211	0.8800959224514157
29	0.2929763016462326	0.889	0.28329507992279046	0.8896882485905044
30	0.2847831801891327	0.8953	0.29433233962713673	0.8812949632188017
31	0.27650256054401395	0.8969	0.25523277032051345	0.8964828137966464
32	0.267156411075592	0.9017	0.25485073733958696	0.8976818545640325
33	0.26312006850242614	0.9048	0.2818355820864606	0.8860911262883462
34	0.2540101212978363	0.907	0.2605440887711126	0.8968824931948209
35	0.24834209787845613	0.9102	0.24614983762530304	0.8996802549853885
36	0.24019949831962586	0.9125	0.24817299116239083	0.8992805748248843
37	0.24335490396022796	0.9114	0.23462710895841357	0.9096722622378934
38	0.23526213252544403	0.914	0.2454405788013594	0.9044764180549328
39	0.23047060565948485	0.9193	0.22966747001397142	0.9112709832610748
40	0.22789492058753968	0.9185	0.2263109861827678	0.914468424449817
41	0.2232737478375435	0.9187	0.22378793935314548	0.9148681047056123
42	0.21766328444480895	0.9228	0.23400407445302113	0.9096722613802726
43	0.21608722815513612	0.9241	0.2203413174211455	0.9156674660748239
44	0.21013811867237092	0.9261	0.23428459241569377	0.9092725811244773
45	0.20506904096603393	0.9261	0.2169585498891098	0.9160671463306193
46	0.2046850640296936	0.9275	0.2186301846643813	0.9176658664961799
47	0.20465559009909629	0.9263	0.22543021848352313	0.9160671454729984
48	0.19537464822530745	0.9311	0.21644848711389622	0.9108713030052795
49	0.19294868628978729	0.9302	0.21683345834533277	0.9188649081211868
50	0.19428436505794525	0.9307	0.21233189522886542	0.918065547609596
51	0.18761416897773742	0.9328	0.2118425979841051	0.9196642686327775
52	0.18689360194206237	0.934	0.21224007509547552	0.9156674660748239
53	0.18278201340436936	0.935	0.21022020874740027	0.9168665068422099
54	0.18697523488998413	0.9341	0.21176324943772895	0.9196642686327775
55	0.1802833846092224	0.9347	0.20964888674463872	0.9172661870980053
56	0.18064909253120423	0.9342	0.20981104917329946	0.9156674660748239
57	0.17643186712265016	0.9351	0.21218147392657927	0.9192645883769821
58	0.17629605107307433	0.938	0.2073085473047839	0.9200639488885729
59	0.1706142538011074	0.939	0.21180891668458254	0.9184652278653914
60	0.17186482763290406	0.9383	0.2289188730952551	0.9140687441940216
61	0.17261638317108155	0.9374	0.20783736706256487	0.9184652278653914
62	0.16398945970535278	0.942	0.20782353354396105	0.9192645883769821
63	0.16389486713409424	0.9394	0.2083204846492679	0.9196642686327775
64	0.1623874683856964	0.9406	0.22899813921176654	0.9116706635168702
65	0.16253190113306046	0.9426	0.2116448327982359	0.9196642686327775
66	0.16146817269921304	0.9425	0.2134553085771396	0.9160671463306193
67	0.15996994326114655	0.9427	0.2149513554361036	0.918065547609596
68	0.1548921233177185	0.9472	0.2069456826011054	0.9224620304233451
69	0.1562265971660614	0.9451	0.2168892097892426	0.9148681055632332
70	0.1535840770125389	0.9448	0.2148958288794227	0.9132693845876972
71	0.14955331642329692	0.9455	0.22448061375047187	0.912470024028461
72	0.15053210363984107	0.9472	0.21673903652399087	0.9148681055632332
73	0.14654843122959138	0.9492	0.21158578326042704	0.9216626699117543
74	0.1442489049077034	0.9482	0.22013622125108942	0.9128697042842563
75	0.14430213849544526	0.949	0.2109166839056545	0.921262989655959
76	0.14244420700073243	0.9497	0.21320841051072334	0.9144684244974626
77	0.14419630343914033	0.9492	0.2115816578066511	0.9144684253550833
78	0.14258800711631775	0.9504	0.20959560583225734	0.9192645883769821
79	0.13624843535423278	0.9493	0.21132131425930348	0.9224620304233451
80	0.13778084468841553	0.9506	0.2200030784824674	0.9148681055632332
81	0.13487489960193633	0.9511	0.21271188725694287	0.9188649073112116
82	0.13048866517543792	0.9526	0.21948142007267257	0.918065547609596
83	0.12856187381744386	0.9556	0.21920265819576623	0.9176658673538006
84	0.12901320819854736	0.9547	0.2190875294837925	0.9176658673538006
85	0.12806423548460008	0.9545	0.21359252338882068	0.9176658673538006
86	0.12725800669193268	0.9551	0.21923340380572967	0.918065547609596
87	0.1260428544998169	0.9569	0.21375420495427008	0.9204636291443682
88	0.12574848392009735	0.956	0.2224878534364948	0.9152677858190285
89	0.12647814896702766	0.9544	0.2251293654564759	0.9156674660748239
90	0.12244717180728912	0.9576	0.2211851463388863	0.9184652278653914
91	0.11639181129932404	0.958	0.22792110732085794	0.9140687450516424
92	0.11928678620755673	0.9585	0.21934279309902832	0.9184652278653914
93	0.11633854884505272	0.9587	0.23206400384696171	0.9144684253074378
94	0.11648701155185699	0.9583	0.2185237642100675	0.9188649073112116
95	0.11588055403232575	0.96	0.2378824975171821	0.911270982403454
96	0.11471660587787628	0.9591	0.2410319657753602	0.9108713021476587
97	0.1129228481054306	0.9605	0.22339905700166163	0.9168665068422099
98	0.11096540366411209	0.9618	0.22467984670071864	0.9172661862880301
99	0.10966427984237671	0.961	0.22642849323203523	0.9168665068422099

The optimal condition:
	epoch: 79
	train_acc: 0.9493
	val_acc: 0.922462030423
	using time: 609.78112483
