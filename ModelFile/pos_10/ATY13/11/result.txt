The number of train datas: 10000
The number of test datas: 2502
epoch	train_loss	train_acc	val_loss	val_acc
0	0.7005747539520264	0.5154	0.6852279294976037	0.5599520383693045
1	0.6913234439849854	0.5322	0.6773695847113356	0.6079136698270778
2	0.6838252104759216	0.5519	0.668884783340015	0.6486810553941033
3	0.6740328323364257	0.581	0.6546369314098435	0.6874500394915696
4	0.6600705129623413	0.6084	0.6368084982525911	0.7046362911578086
5	0.6447722514152527	0.6386	0.6148595091917342	0.7186251001106464
6	0.6244456936836242	0.6539	0.5872336771371934	0.7346123095801314
7	0.6068937773704529	0.6706	0.566029389127553	0.727018385482349
8	0.591673632144928	0.6867	0.5445294155062531	0.7386091120904393
9	0.5761153898239135	0.6988	0.5329138554162164	0.7418065548991318
10	0.5628130481719971	0.7171	0.5145137722400739	0.7597921656475937
11	0.5407573385238648	0.7279	0.49361369554563866	0.7789768188310374
12	0.5322907125473022	0.7354	0.49699546193047395	0.7545963230845835
13	0.5230191610336303	0.7441	0.4645581171809912	0.7973621097400034
14	0.5035283820152283	0.758	0.4513558430685033	0.8065547563856263
15	0.48761242299079893	0.772	0.43993415518535034	0.8045563551066496
16	0.4650629762172699	0.7899	0.40982260694987865	0.8325339722023498
17	0.45127790069580076	0.7956	0.3945847869300537	0.8401278970624617
18	0.4266762111902237	0.8147	0.37158538418993964	0.8573141480616624
19	0.41332841634750367	0.8234	0.3679759935032931	0.8437250192693287
20	0.3966299111366272	0.8309	0.3404046614393056	0.8649080737317495
21	0.38027221937179567	0.8452	0.3322564160509361	0.8689048753844367
22	0.36649353322982786	0.8496	0.3154938878725282	0.8745003990132174
23	0.3580153244972229	0.86	0.3032914191651211	0.8836930457541315
24	0.3441954381942749	0.8621	0.2948390302374113	0.8876898474544644
25	0.3358359760284424	0.8686	0.2862199771818783	0.8916866508223932
26	0.3269876914978027	0.873	0.2810099375286072	0.8980815340098527
27	0.3180030435562134	0.8763	0.2752664020474104	0.8996802550330342
28	0.30845570969581604	0.8805	0.2759598626864614	0.9004796155446249
29	0.30899405379295347	0.8821	0.26973191643361566	0.9008792958004203
30	0.2995370328903198	0.885	0.27385896023848266	0.8988808945214434
31	0.29379946353435515	0.8895	0.25323733710032476	0.9020783366154519
32	0.290529545879364	0.8917	0.2570445094224837	0.9064748193815553
33	0.2821652385234833	0.8943	0.27118251127876536	0.9000799352888295
34	0.27697163863182067	0.8966	0.25146422104583943	0.9076738601489414
35	0.27087827515602114	0.897	0.25505884295458986	0.9068744996373507
36	0.26737089183330537	0.8998	0.2447849486371596	0.9040767378944288
37	0.2664190570950508	0.9011	0.23679779074508414	0.9096722614755638
38	0.25745373911857605	0.9028	0.2402936179551194	0.9096722614279182
39	0.2588364591598511	0.9034	0.2314094082652617	0.913269383777722
40	0.25368662531375885	0.9056	0.2290980244974057	0.913269383777722
41	0.2474493016242981	0.909	0.22622952012897585	0.9124700232661314
42	0.24723864459991454	0.9096	0.23346666782070025	0.914868104753258
43	0.2466502537250519	0.9119	0.22364084890706362	0.914068745099288
44	0.2395278413772583	0.908	0.2265654428661775	0.9144684244974626
45	0.2373667562007904	0.9107	0.21970501103751855	0.9156674661224694
46	0.23317443943023683	0.9157	0.2170307304528501	0.9168665068898555
47	0.2314920888185501	0.9155	0.2278907082968955	0.9140687450516424
48	0.2266042465209961	0.9157	0.21391197531629239	0.9172661863356757
49	0.22481628835201264	0.916	0.22771390726407179	0.9136690647958471
50	0.22345558340549468	0.9202	0.21120779048338784	0.9160671463782648
51	0.2142890240430832	0.9231	0.20926932414039243	0.9176658674014463
52	0.2163837152481079	0.923	0.2079123623794217	0.9180655468472665
53	0.21289246559143066	0.9222	0.2056624331443811	0.9216626699593999
54	0.21346897273063659	0.9224	0.20962271811984046	0.9180655467996208
55	0.2072464581489563	0.9245	0.2131031060533272	0.918065547609596
56	0.20841816368103028	0.923	0.20395651169055753	0.9192645884246278
57	0.20380285105705262	0.9257	0.20200566786656277	0.9204636291920139
58	0.20401175842285157	0.9282	0.20335615674654642	0.9168665060798804
59	0.1996349907040596	0.9278	0.20117918325128029	0.9220623493575745
60	0.20012253968715668	0.9274	0.20551375325897234	0.9232613901249606
61	0.19773909749984742	0.9278	0.19825283143160155	0.9204636291920139
62	0.18872779817581176	0.9314	0.1983296708713809	0.924860111148142
63	0.19197984547615052	0.9311	0.19791385669597714	0.9216626691494247
64	0.18821304488182067	0.9334	0.20510369343318338	0.921262989655959
65	0.18779256496429445	0.9317	0.19580480805356248	0.9244604308923466
66	0.18858770942687988	0.9325	0.19473649127830228	0.9252597914039374
67	0.18664918096065522	0.9314	0.20115524794248274	0.9220623501675497
68	0.181022127532959	0.9343	0.19477181669643268	0.924860111148142
69	0.17983769612312317	0.9375	0.19788951979433414	0.9236610711907312
70	0.1747824264228344	0.9365	0.19324732614125756	0.9264588321713235
71	0.17454136540293694	0.9363	0.20082834372512823	0.9236610711907312
72	0.1765062274336815	0.9355	0.20067152662052334	0.9232613909349358
73	0.1699339584350586	0.9407	0.19181592020985605	0.9252597914039374
74	0.17333925905227662	0.9382	0.19200337169577272	0.924860111148142
75	0.16996350235939026	0.939	0.19243760448660877	0.9264588321713235
76	0.17151443176269532	0.9377	0.19101925517062393	0.9288569137060957
77	0.1690583500623703	0.9393	0.19332401602150057	0.9244604308923466
78	0.16987179815769196	0.9388	0.19119180034962205	0.924860111148142
79	0.16413881845474243	0.94	0.19031753105749424	0.9240607506365514
80	0.1611652304172516	0.945	0.19287175031231463	0.9240607514465264
81	0.1599454464495182	0.9436	0.190196909552379	0.9244604308923466
82	0.15678900215625763	0.9435	0.18940256225714963	0.9264588321713235
83	0.15997148163318634	0.9437	0.19446358762437302	0.9224620304233451
84	0.1533748659133911	0.9462	0.1896970356873376	0.923661070380756
85	0.1545457545220852	0.9447	0.19190903691698513	0.9236610711907312
86	0.15151699533462523	0.947	0.1900437872091548	0.9264588321713235
87	0.1501781569957733	0.9475	0.19157119668263803	0.9220623501675497
88	0.14922140423059463	0.9467	0.19613081278155842	0.9232613909349358
89	0.1463186846256256	0.948	0.19251816968718688	0.9216626691017791
90	0.14908362876176834	0.9476	0.19432491677508748	0.9224620304233451
91	0.1429955892562866	0.9474	0.19089009270941515	0.9228617098691653
92	0.14156026806235314	0.951	0.1901665801803271	0.923661070380756
93	0.13869342048168182	0.9499	0.1919402779875804	0.9208633085901884
94	0.14412366678714753	0.9495	0.19073426490016787	0.9228617098691653
95	0.14166746926307677	0.9516	0.19464927529998058	0.9224620304233451
96	0.1381023084640503	0.9519	0.20044387368966254	0.9200639488885729
97	0.1371868796825409	0.952	0.19073818662136102	0.924860111148142
98	0.13507015013694762	0.9513	0.19351776191990058	0.9244604308923466
99	0.13436155121326446	0.9527	0.19827660398898747	0.921262989655959

The optimal condition:
	epoch: 76
	train_acc: 0.9377
	val_acc: 0.928856913706
	using time: 573.907608986
