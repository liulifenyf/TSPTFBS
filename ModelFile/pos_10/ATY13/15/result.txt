The number of train datas: 10000
The number of test datas: 2502
epoch	train_loss	train_acc	val_loss	val_acc
0	0.7026609637260437	0.5148	0.687036134880319	0.551159072694161
1	0.6905430603027344	0.5344	0.680787033743138	0.580335731414868
2	0.6844311604499816	0.5547	0.6734995137778023	0.6250999195398472
3	0.676336528968811	0.5795	0.6609842271255932	0.656674659747681
4	0.663477538394928	0.5983	0.643782377243042	0.6774580337160783
5	0.6485998698234559	0.6282	0.6216159101775129	0.6922462023228859
6	0.628452481174469	0.6576	0.5944333814030929	0.7166266981169855
7	0.6101696591377258	0.6683	0.5732849706038773	0.7158273374148124
8	0.5914741077423096	0.6865	0.5504638206996887	0.7358113511574926
9	0.5768098994255066	0.6933	0.5387778306464783	0.7358113510622014
10	0.565038190984726	0.7119	0.5215412404992693	0.7529976021566932
11	0.5489325914859772	0.7179	0.505534567754808	0.7657873695798153
12	0.5419629950523377	0.7285	0.5135788737917595	0.7446043158797242
13	0.5341247773170471	0.7354	0.4811183085306276	0.7857713831795586
14	0.5151881458282471	0.7478	0.4688268827019835	0.7897681857375123
15	0.5031242929458618	0.7571	0.4681605512289692	0.7805755398065733
16	0.48422549772262574	0.7726	0.4369474161061928	0.8025579530653432
17	0.46929708261489866	0.7819	0.42125846639716275	0.8129496405259978
18	0.4454099779129028	0.7999	0.40402298677358317	0.8265387683654194
19	0.4336257432937622	0.8084	0.40059507417259554	0.8233413262714109
20	0.41572025113105776	0.8184	0.37671063679585354	0.8381294965934601
21	0.3971067696094513	0.8296	0.3662357521619347	0.8441246995727698
22	0.3838722460746765	0.8427	0.3459384747736936	0.8645083934759541
23	0.36857935152053833	0.8524	0.3295878702216297	0.8697042368489394
24	0.3621454288959503	0.8534	0.3229498360082686	0.872102318336066
25	0.34636294412612917	0.8619	0.3111265723367961	0.8776978419172011
26	0.3309653708457947	0.867	0.30832236354871334	0.8768984813579648
27	0.32417851091623306	0.8694	0.2964449498912604	0.8852917659196923
28	0.31660764570236205	0.8772	0.3139394502535903	0.8760991199411077
29	0.30684108781814573	0.8831	0.297152014385215	0.8876898473591732
30	0.3017543890953064	0.8834	0.2959380258354161	0.8836930448012196
31	0.29155831389427184	0.8856	0.2700816716173951	0.89928057487253
32	0.2842043974399567	0.8917	0.26767784495243163	0.9000799353364751
33	0.2812068705320358	0.8918	0.28797517545360457	0.8868904868475824
34	0.2715079744338989	0.8987	0.27496707054684394	0.8936850511961036
35	0.2633424043178558	0.9041	0.2724943193290636	0.8920863301729222
36	0.26089702105522156	0.9004	0.25456012669894146	0.9036770585438998
37	0.25704823637008667	0.9029	0.2502907054077426	0.9088729017739483
38	0.2523934973716736	0.9015	0.2548072342868808	0.9008792958004203
39	0.24843554887771607	0.9068	0.24740145420380158	0.9072741798931461
40	0.2455626300573349	0.9099	0.24497908748072877	0.908073540500028
41	0.23800760358572007	0.9135	0.24011743155648288	0.9112709833087205
42	0.2362385353088379	0.9124	0.24911567803433568	0.9024780168236016
43	0.23546980600357056	0.9128	0.23691850892312044	0.9104716227971297
44	0.2286715950012207	0.9164	0.2513079669573705	0.9032773773351924
45	0.22328955960273741	0.9176	0.238617390846844	0.9076738610065622
46	0.2182400918006897	0.9201	0.23046132274192396	0.9100719425413344
47	0.2183542947769165	0.9171	0.2425873516906175	0.90607513912576
48	0.21080567626953126	0.9216	0.2308605430032805	0.9104716228447753
49	0.2094699562072754	0.9219	0.23554517964807917	0.9092725820297436
50	0.20743715279102326	0.9271	0.22582150317972702	0.9136690648434926
51	0.2025009175300598	0.9238	0.22824406279600876	0.912869704331902
52	0.20166988859176635	0.9247	0.22587673226706415	0.9112709833087205
53	0.19918676327466964	0.9282	0.22299652915302035	0.9124700240761066
54	0.20071916146278382	0.9273	0.22664132909618503	0.9128697034742811
55	0.19516741704940796	0.9297	0.2288317939014934	0.9120703429626904
56	0.1924938609600067	0.9272	0.22421907498348626	0.9120703438679568
57	0.1898114984512329	0.9317	0.22750174970054132	0.9120703437726656
58	0.18860942041873932	0.9311	0.2207328546390259	0.9176658665914711
59	0.18454457111358644	0.9332	0.21893876109668295	0.9152677858666741
60	0.18588081591129302	0.932	0.2251543770019385	0.9116706635168702
61	0.1821901213645935	0.9352	0.22168338008159452	0.9132693846353429
62	0.17654130930900575	0.9386	0.21775698253243184	0.916067145520644
63	0.17886152873039246	0.936	0.21758766394915532	0.9172661871456509
64	0.1763437731742859	0.9384	0.23127183874400495	0.9096722622378934
65	0.1765485919237137	0.9367	0.22114257021940392	0.9132693845400517
66	0.1752732519865036	0.9377	0.21665850590220648	0.9172661870980053
67	0.17311841769218445	0.9383	0.22526445681695267	0.9116706635168702
68	0.1680771171092987	0.9383	0.21727827060327445	0.9156674660748239
69	0.16935582686662673	0.9404	0.22290141224789678	0.912470024028461
70	0.16343577778339385	0.943	0.2265442625867377	0.9088729010116187
71	0.16270557831525803	0.9405	0.2208612919044342	0.9160671463306193
72	0.1615049734354019	0.9416	0.2361123347870833	0.9076738601012958
73	0.1584388624191284	0.9424	0.21530349665789678	0.9180655476572417
74	0.1577745180606842	0.9442	0.2243581873704966	0.9144684253074378
75	0.1565172484636307	0.9454	0.21734184141067578	0.9180655476572417
76	0.15552207736968995	0.9451	0.21634047911416807	0.9184652278653914
77	0.15777955451011658	0.9452	0.21551353700584072	0.9192645883769821
78	0.15636272931098938	0.9437	0.2146879230662406	0.9196642686327775
79	0.15159058322906493	0.9456	0.21517164607366307	0.918065547609596
80	0.14980324873924256	0.9465	0.2148262000412678	0.9192645883769821
81	0.14759962354898454	0.9462	0.21886986963373484	0.918065547609596
82	0.14221759943962098	0.9511	0.2177261234306508	0.9176658673538006
83	0.1438374636888504	0.9508	0.221587649906139	0.9168665059845892
84	0.1424550539493561	0.9512	0.2200106249021874	0.9172661862403845
85	0.14217840550243854	0.9484	0.22417900285465445	0.9168665059845892
86	0.1431647897720337	0.9499	0.22265834138571597	0.9156674652172031
87	0.13860668096542358	0.9501	0.21543822484574826	0.9200639488885729
88	0.13656055410504342	0.9509	0.225990969732368	0.9164668257287938
89	0.13745266233682632	0.951	0.2299062205340079	0.9136690639382262
90	0.1381967449903488	0.9499	0.23086854645745647	0.9116706626592495
91	0.1316651075363159	0.9539	0.22629749564696655	0.9136690639382262
92	0.13231593424081803	0.9546	0.22089993614229939	0.9160671454729984
93	0.12834770090579986	0.9537	0.23237163431162267	0.9140687441940216
94	0.1280046124458313	0.9561	0.2195471375465965	0.9176658673538006
95	0.13060422834157945	0.9552	0.22946852263953568	0.9136690639382262
96	0.12433661847114563	0.9557	0.22622307454891724	0.9164668257287938
97	0.12598366644382478	0.9568	0.22637686195323983	0.9120703438203112
98	0.12369305999279022	0.9556	0.22420111546318214	0.914868104753258
99	0.12371719170808793	0.9557	0.23145168201385927	0.9156674652172031

The optimal condition:
	epoch: 87
	train_acc: 0.9501
	val_acc: 0.920063948889
	using time: 615.365953207
