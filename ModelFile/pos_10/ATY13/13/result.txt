The number of train datas: 10000
The number of test datas: 2502
epoch	train_loss	train_acc	val_loss	val_acc
0	0.7014193532943725	0.5125	0.6873449251043806	0.5555555563655308
1	0.6917029530525207	0.5298	0.6802039030168078	0.5983213436403435
2	0.685265350818634	0.5505	0.6734647199118452	0.6354916071434387
3	0.6776657348632813	0.5751	0.6611756722418238	0.6714628293550462
4	0.665774172115326	0.603	0.6462453547046244	0.6954436445121857
5	0.6511158631324768	0.6273	0.6246494451205699	0.7174260593909059
6	0.6318393135070801	0.6519	0.5962612058618944	0.7318145478372093
7	0.612376794910431	0.6727	0.5738560214793558	0.7262190249707583
8	0.5957396677017212	0.6846	0.5490453622514586	0.7418065541844479
9	0.576382788181305	0.6995	0.5332765916792704	0.73940847260203
10	0.5648833450317383	0.7106	0.5151416570972577	0.7681854510669418
11	0.5420506834983826	0.7286	0.494414863898028	0.7809752192523935
12	0.5349211800575256	0.7353	0.4956766081799706	0.7573940840175303
13	0.5244214719772339	0.7415	0.4657350716878661	0.8001598715782166
14	0.5051168678283692	0.7559	0.4503754331625337	0.8053557148559107
15	0.4881585868835449	0.7727	0.439665896000622	0.810151878640139
16	0.4631101655006409	0.7901	0.4094550189116209	0.8333333327139405
17	0.44969192142486575	0.7973	0.39451413758271797	0.8429256596153588
18	0.42194584503173826	0.8166	0.371474547661561	0.8541167067776291
19	0.4082625036239624	0.8237	0.35865587970430995	0.8637090320114514
20	0.3897355679988861	0.8363	0.3391876359590047	0.8717026371273586
21	0.37460544190406797	0.8472	0.3261762704732988	0.8768984804526984
22	0.3580720242023468	0.8561	0.31204417995888173	0.8796962431008867
23	0.3460270278930664	0.8621	0.2959329490181353	0.8836930448488651
24	0.3374052488327026	0.8692	0.2875101880180083	0.8912869705189522
25	0.3228601314544678	0.8745	0.27746448780802324	0.8936850520537245
26	0.3113876640796661	0.8809	0.2719771077426122	0.896482813844292
27	0.30127943816185	0.8841	0.2636731344161274	0.9000799361464503
28	0.29496562633514406	0.8858	0.27787501248214647	0.890487609959716
29	0.2905640026211739	0.8916	0.2586029481997402	0.9004796163546
30	0.28072063865661623	0.8941	0.2639758653587384	0.8976818545640325
31	0.2733305370569229	0.8978	0.24066255678185267	0.9076738610065622
32	0.2689935406684876	0.9013	0.2412925163666598	0.9064748193815553
33	0.26368516263961794	0.9037	0.24955702014416337	0.9036770575433422
34	0.2597119772911072	0.9048	0.24153667196428938	0.9088729008686819
35	0.25015152547359465	0.9082	0.246950152013704	0.9036770583533174
36	0.24956757209300995	0.9094	0.2280348246451095	0.9060751392210511
37	0.24722104552984236	0.91	0.22399339434816587	0.9100719416837135
38	0.24020740599632262	0.9109	0.2265031994353953	0.9136690647482014
39	0.238482666683197	0.9128	0.21849645927941486	0.9132693845400517
40	0.23683446187973023	0.9141	0.21707816255226028	0.9164668257287938
41	0.22894708969593047	0.9152	0.21225604413748742	0.916067145520644
42	0.22924790358543395	0.9151	0.21496750661533037	0.9144684252597922
43	0.22608695437908172	0.9223	0.2103968866520839	0.9160671462829736
44	0.2212699648141861	0.9185	0.21157871915710916	0.9160671462829736
45	0.21564438047409057	0.9218	0.2062638287278388	0.9196642677751568
46	0.2157877965450287	0.9249	0.20407252891553487	0.9192645875193614
47	0.21163804895877839	0.9239	0.2163170222207892	0.9172661870503597
48	0.2051671887397766	0.9258	0.20247308320278745	0.9176658673538006
49	0.20228610743284225	0.927	0.20761879331035485	0.919664268585132
50	0.20375144075155258	0.928	0.20042331746037154	0.9184652270077707
51	0.198512002491951	0.9248	0.19935879513895294	0.9192645875193614
52	0.1983020544052124	0.927	0.198661929328474	0.918065547609596
53	0.19419167609214782	0.9305	0.196545477250783	0.9196642677751568
54	0.19636718544960022	0.9299	0.20001835954322708	0.9212629896083133
55	0.1919685456752777	0.9291	0.19898108881916837	0.9200639488409272
56	0.19196048941612243	0.932	0.19577691486651758	0.9184652270077707
57	0.18590012605190276	0.9314	0.1958680973469401	0.9192645875193614
58	0.18622211154699325	0.9342	0.19452517185446552	0.9204636282867474
59	0.18281108926534653	0.9342	0.19480354407374903	0.9192645875193614
60	0.18808539700508117	0.9323	0.2041990065293537	0.9180655475619505
61	0.18216764965057372	0.9355	0.1934788735221616	0.9208633085901884
62	0.17402224445343017	0.9391	0.1953654684168067	0.9184652278177458
63	0.17609498443603516	0.9387	0.19266501080980308	0.9220623501675497
64	0.17509893786907196	0.9374	0.2051287124917614	0.9204636290967226
65	0.17333761649131774	0.9386	0.19399217861018878	0.919664268585132
66	0.1721032334089279	0.9391	0.19184470969400436	0.9224620303756994
67	0.1710574401855469	0.9376	0.20470680515888592	0.9176658673061551
68	0.1688647421836853	0.9405	0.19072783950993197	0.9236610703331103
69	0.16774316186904908	0.9407	0.19779690711511125	0.9188649080735412
70	0.16359832310080527	0.9411	0.19156081717815712	0.9244604308923466
71	0.16447580506801604	0.9397	0.19315317803197246	0.920863309352518
72	0.16218522495627402	0.9418	0.20583319855477217	0.9136690647482014
73	0.1595784861087799	0.9428	0.19040823820755065	0.9240607514465264
74	0.159459086227417	0.9431	0.19600269207351215	0.9212629896083133
75	0.15756165587902068	0.9451	0.19104262899985608	0.9252597922139126
76	0.15805687341690064	0.9436	0.19369562136397944	0.9224620296133699
77	0.15957617835998536	0.9441	0.19014431881032687	0.9248601119581172
78	0.15758891003131867	0.9456	0.19069327767208802	0.925259792166267
79	0.15423368635177612	0.9435	0.18885331005930042	0.9260591527255033
80	0.15260917851924896	0.946	0.18939410507988683	0.9264588321236779
81	0.1509616297006607	0.9453	0.1885814075525716	0.9260591527255033
82	0.1477438814163208	0.9463	0.1897615537130766	0.9236610711907312
83	0.14754456419944764	0.9479	0.19121837168693734	0.925259792166267
84	0.144766965508461	0.9471	0.1900535731149806	0.9260591527255033
85	0.14529215961694716	0.9489	0.19122144455413262	0.9232613908872902
86	0.1439569257736206	0.9517	0.19459970065539212	0.9220623501199041
87	0.1419401319026947	0.9502	0.19147959851818405	0.9256594724220624
88	0.13992845643758775	0.9497	0.19806255277791182	0.920863309352518
89	0.14067221777439118	0.9509	0.20176039408627364	0.9180655475619505
90	0.14282114081382752	0.9483	0.20059804340822995	0.9180655475619505
91	0.13496243472099304	0.9499	0.19378456346791903	0.9224620303756994
92	0.1368170221686363	0.9507	0.19080535770891954	0.9256594724697079
93	0.1332709281682968	0.953	0.19740141904968725	0.919664268585132
94	0.13555719769001007	0.9534	0.1919096454882698	0.9264588329812987
95	0.1343622605919838	0.9538	0.19870801907863547	0.9224620303756994
96	0.13207458519935608	0.9544	0.19947589870444019	0.9220623501199041
97	0.13034964179992675	0.955	0.1930851278414162	0.926858513237094
98	0.1300370908021927	0.9538	0.1931344479810801	0.9252597914039374
99	0.1275770490705967	0.9541	0.1961261642595037	0.9228617106791404

The optimal condition:
	epoch: 97
	train_acc: 0.955
	val_acc: 0.926858513237
	using time: 591.421391964
