The number of train datas: 10000
The number of test datas: 2502
epoch	train_loss	train_acc	val_loss	val_acc
0	0.7037405820846557	0.5033	0.68916911043995	0.5319744204874519
1	0.6921970983505249	0.5219	0.6827364143707769	0.5807354117183091
2	0.6854297973632812	0.5483	0.6749840166738375	0.6247002401893182
3	0.6751303819656372	0.5789	0.6613044660630748	0.6622701842340825
4	0.6620524722099305	0.6076	0.6423612248888023	0.6770583527932446
5	0.6470640736579895	0.6301	0.6172291759392626	0.6926458827692638
6	0.6243649796485901	0.6604	0.586569160485058	0.7106314943277007
7	0.602866551399231	0.6793	0.5684626316376251	0.7134292567853067
8	0.5871841029167175	0.6885	0.5465665475832377	0.7290167860466418
9	0.574738150215149	0.6971	0.5396839341671347	0.7270183847200193
10	0.5664287406921387	0.7067	0.5228614563659894	0.7490007988840556
11	0.5523570890426636	0.7176	0.5089367131630389	0.7585931250231444
12	0.5450661108016968	0.7257	0.5161132129500333	0.7410071944828324
13	0.5360342579841614	0.7318	0.48828298506214557	0.779376499182124
14	0.5188301054000855	0.746	0.47618808965984105	0.7869704239945904
15	0.508505096912384	0.7526	0.48097350883731643	0.7653876900863495
16	0.4916941282272339	0.7678	0.4509836186846193	0.8041566741361702
17	0.47754700288772584	0.7777	0.43379567931596036	0.8177458028332125
18	0.45775772619247435	0.7912	0.41869128684250473	0.8213429251353709
19	0.44529002294540404	0.7968	0.40991155020624614	0.8213429258977004
20	0.4265524320602417	0.8124	0.38974029466593196	0.8325339730599706
21	0.40513643703460694	0.828	0.37114849203019784	0.84292565971065
22	0.39272835063934325	0.8348	0.35968912076607024	0.8501199043149666
23	0.3792304023742676	0.8404	0.3427772618454995	0.8609112712214414
24	0.3644383391857147	0.8509	0.3306310912020963	0.864908073779395
25	0.3532067518234253	0.8558	0.32701862939923027	0.8605115909656461
26	0.33964711866378783	0.8637	0.315779665176817	0.8681055157781123
27	0.32460470371246336	0.8716	0.3038159510929236	0.872102318336066
28	0.3181949744224548	0.873	0.33360597996307695	0.858912868941907
29	0.3101841365814209	0.8796	0.29268960856038223	0.8788968817793208
30	0.3011846966266632	0.8835	0.3337336728493754	0.8577138281745209
31	0.2951176313638687	0.8849	0.2713369041990986	0.8916866500124181
32	0.28018939452171326	0.8919	0.26746992246328977	0.8908872904060937
33	0.2748552783250809	0.8974	0.3160579202415274	0.8701039161041773
34	0.26941609654426574	0.9008	0.26733473187489665	0.8932853717979291
35	0.2617618793964386	0.9014	0.2546488299406023	0.9000799361464503
36	0.2558461970806122	0.904	0.25794074573009895	0.8948840921540722
37	0.24983892729282378	0.909	0.2459255849762405	0.9012789761515068
38	0.2452994703769684	0.9087	0.24353433348577944	0.9052757786618149
39	0.2434593906402588	0.909	0.23985871098500838	0.9084732207558234
40	0.23966862802505493	0.9144	0.23894598012824328	0.9048760984060195
41	0.2305253043413162	0.917	0.23665984452580757	0.9072741799407916
42	0.2291364806175232	0.916	0.2531752230094777	0.8984812150756232
43	0.22445453386306763	0.9192	0.23225314316036794	0.909672262285539
44	0.2202870050907135	0.9198	0.24445378562386375	0.9056754596799398
45	0.21492900896072387	0.9183	0.24636824539811206	0.9044764189125537
46	0.21535469808578492	0.9226	0.2263364715875386	0.9104716219871546
47	0.2110883692741394	0.9224	0.22781684571842878	0.9132693845400517
48	0.20420453462600707	0.9281	0.22558993346494832	0.9132693837300765
49	0.20048358488082885	0.9281	0.23863633857261268	0.9088729017263026
50	0.2003005352497101	0.9301	0.22454592976257573	0.9112709832610748
51	0.19489805097579957	0.9305	0.2251722080935296	0.9128697042842563
52	0.19584924449920654	0.931	0.23604958887389904	0.9076738603395238
53	0.19383004138469695	0.9317	0.22440505543534608	0.9160671463306193
54	0.19011575663089753	0.9321	0.22398088272098157	0.9148681055632332
55	0.18655931832790373	0.9348	0.22232744020285558	0.9136690647958471
56	0.18627189712524414	0.933	0.23790070195373395	0.9068745006855539
57	0.18239498181343078	0.9332	0.22056996829504016	0.9156674660748239
58	0.18116782454252242	0.9362	0.2245734131259979	0.9132693845400517
59	0.17678703147172928	0.938	0.2228377652730492	0.9156674652648487
60	0.17927506852149963	0.9334	0.2464692782250335	0.9052757794241444
61	0.17534104356765748	0.938	0.22722717323939767	0.9116706636121614
62	0.17265617456436158	0.9386	0.22327317570229704	0.9156674652648487
63	0.17148099241256715	0.9424	0.22216932321909807	0.9136690639858719
64	0.1690918520450592	0.9401	0.23410303530980833	0.912470024028461
65	0.16614457615613937	0.9426	0.22299289335306885	0.9136690648434926
66	0.1654907371520996	0.9416	0.23282360589856818	0.9140687450516424
67	0.16499557919502258	0.9419	0.23011062480658173	0.9156674660748239
68	0.16320407881736756	0.9426	0.22279766143607102	0.9140687442416672
69	0.1585478326559067	0.9476	0.23622582597936467	0.9108713030052795
70	0.15438080568909646	0.9456	0.2296832155504768	0.9100719417790047
71	0.15511579164862632	0.9453	0.24148437187230462	0.9100719424936887
72	0.15501344800591468	0.9459	0.2521759550586688	0.9060751399357351
73	0.15282472915649414	0.9452	0.23830369098199833	0.9064748203821129
74	0.15119642795324326	0.9464	0.2371930467972843	0.9112709824510997
75	0.14677865608930588	0.9465	0.23827031869396603	0.904476419103136
76	0.14445237431526184	0.9478	0.24705763607859899	0.9020783375207183
77	0.15012422380447388	0.9489	0.2609594956719332	0.8928856909703865
78	0.14759009153842925	0.951	0.23001225392738406	0.9132693838253677
79	0.14391815342903136	0.9499	0.22903273051543582	0.9128697044271931
80	0.14213916630744933	0.9491	0.23094891230884693	0.9124700240761066
81	0.1378574542403221	0.9503	0.23093949051069126	0.9136690648911382
82	0.13733051347136496	0.9531	0.23207993161001747	0.912070343010336
83	0.13591685028672218	0.9544	0.23332813159405566	0.9112709833087205
84	0.13434080262184142	0.9548	0.2354818440550904	0.9092725811721228
85	0.13126699158847333	0.9552	0.23123180913887054	0.911270983356366
86	0.1318776214122772	0.9555	0.24391819651035382	0.9040767386567583
87	0.13221282997131348	0.9548	0.23189214400822025	0.9124700241237521
88	0.13017883970737457	0.9547	0.23833213529760222	0.9080735404047368
89	0.12946043763160706	0.9547	0.24253843996307548	0.9104716227494841
90	0.1283814976811409	0.9557	0.2560430354566025	0.904876099168349
91	0.12210224633216858	0.9567	0.24417316304932205	0.9088729017263026
92	0.12330126452445984	0.9562	0.2369162919376489	0.913269383777722
93	0.11941080119609833	0.9589	0.25721117376471214	0.9056754596799398
94	0.12128031988143921	0.9588	0.25580172706469834	0.9028776980799547
95	0.12173585917949677	0.9563	0.24059421930619948	0.9088729009639731
96	0.11473560613393784	0.9626	0.24304817732480122	0.9108713030529251
97	0.11444433717727662	0.962	0.24490966039786427	0.9072741799884372
98	0.11331276772022247	0.9613	0.24560631758017507	0.9044764182455153
99	0.11197583710551262	0.9622	0.2575653387214259	0.9092725811244773

The optimal condition:
	epoch: 53
	train_acc: 0.9317
	val_acc: 0.916067146331
	using time: 535.962165833
